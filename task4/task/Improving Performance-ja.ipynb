{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# モデルを改善"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを学習する方法を学んだので、次はモデルを洗練させましょう。\n",
    "このラボでは、ディープラーニングを実際に利用する際に、目指す結果を達成するための手段を学習します。\n",
    "このプロセスでは、まず、これを実現するテクノロジを明らかにします。\n",
    "犬/猫の分類器に戻りましょう。\n",
    "\n",
    "## [DIGITSのホーム画面を開く](/digits/)\n",
    "\n",
    "作成したモデル「Dogs vs. Cats」を選択します。\n",
    "\n",
    "DIGITS に、モデルのトレーニング時に生成されたグラフが他の要素と共に表示されます。\n",
    "\n",
    "![](images/graphfromfirsttraining.png)\n",
    "\n",
    "学習損失、検証損失、精度の 3 つの量がレポートされています。\n",
    "学習および検証の損失の値は、多少の上下はあるものの、エポック間で減少しているはずです。\n",
    "精度は、モデルが検証データを正しく分類する能力の指標です。\n",
    "データポイントにマウスを合わせると、正確な値が表示されます。\n",
    "このケースでは、最後のエポックの精度は約 80% です。\n",
    "初期のネットワークはランダムに生成されているため、皆さんの結果は、ここで示すものとは少し異なっている可能性があります。\n",
    "\n",
    "このグラフを分析してわかることは、精度が徐々に上がり、損失が徐々に減少するということです。\n",
    "ここで、「学習を長く行えばモデルは改善され続けるのか」という当然の疑問が出てきます。\n",
    "これが、私たちが実験し、議論する最初の変更内容になります。\n",
    "\n",
    "## もっと学習を\n",
    "\n",
    "子供を指導する世の親御さんや先生たちにならって、モデルにもっと勉強させ、精度が上がるよう取り組んでみましょう。\n",
    "\n",
    "ひととおりのデータをネットワークに 1 回学習させることを、1 **エポック** といいます。\n",
    "このコースの前半で、1 エポックをフラッシュカードの山 1 つを 1 回見ることにたとえました。\n",
    "「もっと学習する」ということは、モデルが前回学習を終えたところから、さらに多くのエポックを実行するということです。\n",
    "\n",
    "その *状態* にアクセスするには、モデルページの一番下までスクロールし、\"Make Pretrained Model\" という大きな緑のボタンをクリックします。\n",
    "\n",
    "![](images/pretrainedbutton.PNG)\n",
    "\n",
    "これにより、以下の 2 つが保存されます。\n",
    "\n",
    "1. \"AlexNet\" の選択時に選んだ「ネットワークアーキテクチャ」。\n",
    "2. モデルが「学習」した内容。最初の 5 エポックでデータを用いて調整されたパラメーターが保存される。\n",
    "\n",
    "### 学習率\n",
    "\n",
    "モデルの現在の状態に加えて、これがどう変化していたかを理解することも重要です。学習グラフの下にあるグラフを見てください。\n",
    "\n",
    "![](images/learningrate.png)\n",
    "\n",
    "以下の 3 つについて説明します。\n",
    "\n",
    "1) 「学習率」とは何か。  \n",
    "2) 学習中、学習率が減少しているのはなぜか。  \n",
    "3) だれがこれをコントロールするか。  \n",
    "\n",
    "\n",
    "1) 学習率とは、学習中に各「重み」を変化させる比率です。各重みは、学習率をかけた値の分だけ、損失を減少させる方向に変化します。  \n",
    "2) ネットワークが理想の解に近づいているため、学習の進行に伴って学習率を減少させます。学習中のネットワークは、最初は与えられたデータについて *何も* 知りません。見ているデータが犬かどうかはもちろん、画像かセンサーデータかさえもわかりません。理想的な解に向けて大きく変化するのは自然です。数エポック後、重みは徐々に良くなっていきます。そのため、ネットワークが各画像に対して、より小さな反応しかしなくなることは重要です。  \n",
    "3) 学習率をコントロールするのは皆さんです。学習率は、学習のセットアップ時に設定する多数の「ハイパーパラメーター」の 1 つです。この後、調整を加えていきます。これがこのラボの狙いです。  \n",
    "\n",
    "セッションの終わりに学習率は小さくなっているため、事前学習済みのネットワークから開始する場合は、前回の学習終了時の学習率を選択する必要があります。この事前学習済みモデルから、学習率 0.0001 で始まる学習ジョブをセットアップしましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前学習済みのモデルからはじめる\n",
    "\n",
    "このスタート地点から、新しいモデルを作成することができます。\n",
    "左上隅の \"DIGITS\" を選択して DIGITS のホーム画面に戻り、以前と同じように新しい画像分類モデルを作成します。\n",
    "\n",
    "**\"New Model\" (Images) -> \"Classification\" **\n",
    "\n",
    "- 同じデータセット (Dogs and Cats) を選択します。\n",
    "- エポックを 3 ～ 8 のいずれかの値に設定します (モデルをゼロから作成する場合、より多くのエポックを指定することもあるでしょう)。\n",
    "- 学習率を 0.0001 に変更します。\n",
    "- \"advanced options\" で \"Fixed\" を選択して、学習率の値を固定します。\n",
    "- 今回は、\"Standard Network\" を選択する代わりに、\"Pretrained Networks\" を選択します。\n",
    "- 先ほど作成した事前学習済みモデル「Dogs vs. Cats」を選択します。\n",
    "- モデルに名前を付けます。ここでは「Study more」としています。\n",
    "- \"Create\" をクリックします。\n",
    "\n",
    "設定は以下のようになります。\n",
    "\n",
    "![](images/pretrainedmodelsetup.png)\n",
    "\n",
    "モデルを作成すると、以下のようなグラフを確認できます。\n",
    "\n",
    "![](images/pretrainedgraph.PNG)\n",
    "\n",
    "以下の点に注目してください。\n",
    "\n",
    "1. 期待どおり、精度は最初のモデルの学習終了時に近い 80% から開始されます。\n",
    "2. 精度は向上し続け、エポック数を増やすことで、多くの場合、パフォーマンスが向上することが示されています。\n",
    "3. 精度の向上 *率* は低速になります。これは、同じデータの学習回数を増やすことだけがパフォーマンスを向上する方法ではないことを示しています。\n",
    "\n",
    "パフォーマンスを向上させるための要素は 4 種類あります。\n",
    "それぞれ時間をかけて学習することで、モデルのパフォーマンスが向上します。\n",
    "\n",
    "1) **データ** - 十分な大きさと多様性を持ったデータセットがあれば、モデルが有効に機能する環境が整います。データキュレーションは、それ自体が 1 つの技術です。  \n",
    "2) **ハイパーパラメーター** - 学習率などのオプションを変更することは、学習の「スタイル」を変更することに似ています。現時点では、適切なハイパーパラメーターの特定は、実験を通じて身に付ける手動のプロセスです。どのハイパーパラメーターがどの種類のジョブに効果的かを知ることが、パフォーマンスを向上させる助けになります。  \n",
    "3) **学習時間** - ある時点までは、エポック数が多いほどパフォーマンスが向上します。多く学習しすぎると、ある時点で過剰適合が発生します (これは人間にも責任があります)。このため、単体で調整することはできません。  \n",
    "4) **ネットワークアーキテクチャ** - 次のセクションで、ネットワークアーキテクチャの実験を始めます。この最後の調整項目は、ディープラーニングによる問題解決に取り組むには、ネットワークアーキテクチャに精通している必要があるという間違った認識に異議を唱えるために挙げました。この分野はとても魅力的で強力です。スキルを向上するには数学を勉強する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 著名なモデルをデプロイ\n",
    "\n",
    "再学習したモデルをデプロイする代わりとなる、パフォーマンス向上への近道を紹介しましょう。\n",
    "それは、専門家が事前に学習したモデルをデプロイすることです。\n",
    "\n",
    "このセクションでは、他の人のネットワークをデプロイし、その研究、計算時間、データキュレーションから得られるパフォーマンスの向上を実現する方法を学びます。\n",
    "\n",
    "このコースの最初に使用したディープラーニングワークフローは、 *画像分類* であったことを思い出してください。\n",
    "このタスクから開始する理由の 1 つは、これがディープラーニングで最も多く解決されている課題の 1 つであるためです。\n",
    "「[ImageNet](https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/)」と呼ばれるコンテストのために研究コミュニティが洗練させたソリューションの恩恵を受けることができます。\n",
    "\n",
    "「ImageNet」は、1,000 カテゴリの一般的な画像が含まれた大きなデータセットです。\n",
    "このコンテストでは、このデータセットに対して最小の損失を実現した研究チームに賞が与えられます。\n",
    "このコースで使用しているネットワーク AlexNet は、2012 年に ImageNet で勝利しました。\n",
    "それ以降は、Google および Microsoft のチームが勝者となっています。\n",
    "\n",
    "すばらしいことに、皆さんもこれらのネットワークアーキテクチャを適用できるだけではなく、データ、ハイパーパラメーター、学習時間、ネットワークアーキテクチャという 4 つの要素を操作して獲得された学習済みの重みを使用できます。\n",
    "学習やデータ収集を行わずに、優勝したニューラルネットワークを *デプロイ* できるのです。\n",
    "\n",
    "これらのモデルをデプロイするのに必要なのは、モデルのアーキテクチャと重みだけです。\n",
    "Google で「事前学習済みモデル alexnet imagenet caffe」と検索すると、このモデルをダウンロードできるページが複数見つかります。\n",
    "\n",
    "ここでは wget と呼ばれるツールを使用して、この 2 つをダウンロードします。\n",
    "wget は、最初にローカルマシンにダウンロードしなくても、使用しているサーバーに Web から直接データをダウンロードできる便利なツールです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel\n",
    "!wget https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_alexnet/deploy.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 つのファイルは、モデルをゼロから学習したときに DIGITS で生成されたものと同じファイルです。\n",
    "他に DIGITS から取得したファイルは、学習時に使用した平均画像だけでした。\n",
    "これは以下のようにダウンロードできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/BVLC/caffe/blob/master/python/caffe/imagenet/ilsvrc_2012_mean.npy?raw=true\n",
    "!mv ilsvrc_2012_mean.npy?raw=true ilsvrc_2012_mean.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで 3 つのファイルすべてが、このサーバーに以下のように格納されました。\n",
    "\n",
    "<pre>/dli/tasks/task4/task/deploy.prototxt</pre>\n",
    "<pre>/dli/tasks/task4/task/bvlc_alexnet.caffemodel</pre>\n",
    "<pre>/dli/tasks/task4/task/ilsvrc_2012_mean.npy</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デプロイに関する既知の内容を利用して、このモデルに画像を入力しましょう。\n",
    "まずは、次のようにして、モデルを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import caffe\n",
    "import numpy as np\n",
    "caffe.set_mode_gpu()\n",
    "import matplotlib.pyplot as plt #matplotlib.pyplot allows us to visualize results\n",
    "\n",
    "ARCHITECTURE = 'deploy.prototxt'\n",
    "WEIGHTS = 'bvlc_alexnet.caffemodel'\n",
    "MEAN_IMAGE = 'ilsvrc_2012_mean.npy'\n",
    "TEST_IMAGE = '/dli/data/BeagleImages/louietest2.JPG'\n",
    "\n",
    "# Initialize the Caffe model using the model trained in DIGITS\n",
    "net = caffe.Classifier(ARCHITECTURE, WEIGHTS) #Each \"channel\" of our images are 256 x 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、ネットワークで想定されている入力を作成します。\n",
    "これは前回のモデルで使用された *前処理* とは異なります。\n",
    "ImageNet がどのように前処理されているかを知るには、[Caffe Web サイト](http://caffe.berkeleyvision.org/gathered/examples/imagenet.html) で提供されているドキュメントを参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the image\n",
    "image= caffe.io.load_image(TEST_IMAGE)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "#Load the mean image\n",
    "mean_image = np.load(MEAN_IMAGE)\n",
    "mu = mean_image.mean(1).mean(1)  # average over pixels to obtain the mean (BGR) pixel values\n",
    "\n",
    "# create transformer for the input called 'data'\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension\n",
    "transformer.set_mean('data', mu)            # subtract the dataset-mean value in each channel\n",
    "transformer.set_raw_scale('data', 255)      # rescale from [0, 1] to [0, 255]\n",
    "transformer.set_channel_swap('data', (2,1,0))  # swap channels from RGB to BGR\n",
    "# set the size of the input (we can skip this if we're happy with the default; we can also change it later, e.g., for different batch sizes)\n",
    "net.blobs['data'].reshape(1,        # batch size\n",
    "                          3,         # 3-channel (BGR) images\n",
    "                          227, 227)  # image size is 227x227\n",
    "\n",
    "transformed_image = transformer.preprocess('data', image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数を実行し、出力を可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the image data into the memory allocated for the net\n",
    "net.blobs['data'].data[...] = transformed_image\n",
    "\n",
    "### perform classification\n",
    "output = net.forward()\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力を有用な形にする\n",
    "\n",
    "上に表示されているのは、画像が 1,000 のカテゴリのそれぞれに属している確率が格納された配列です。\n",
    "これを利用しやすい形に変えましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prob = output['prob'][0]  # the output probability vector for the first image in the batch\n",
    "print 'predicted class is:', output_prob.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一歩近付きました。\n",
    "[ここ](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a) から ImageNet のカテゴリと、どの数値が対応しているかを確認してください。\n",
    "良くなっていますか？\n",
    "この機能をアプリケーションに追加して、便利なエンドツーエンドのデプロイを実現しましょう。\n",
    "\n",
    "再び wget を使用して、ラベル付けするカテゴリのディクショナリ (dict) を取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/HoldenCaulfieldRye/caffe/master/data/ilsvrc12/synset_words.txt\n",
    "labels_file = 'synset_words.txt'\n",
    "labels = np.loadtxt(labels_file, str, delimiter='\\t')\n",
    "\n",
    "print 'output label:', labels[output_prob.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アプリケーションがどのような処理を行っているのかをわかりやすくするために、以下のようにして、アプリケーションの入力と出力を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Input image:\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "print(\"Output label:\" + labels[output_prob.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
