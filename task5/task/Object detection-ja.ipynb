{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 物体検出\n",
    "\n",
    "### デプロイの活用\n",
    "\n",
    "スライディングウィンドウを用いて物体を検出しようとすることは、1 枚の画像の 256 x 256 部分を「dog」または「cat」として分類する画像分類器を、一度にひとつの 256 x 256 領域を処理するアプリケーションへデプロイすることを含みます。\n",
    "これは完璧なソリューションではありませんが、以下を行います。\n",
    "\n",
    "1) ディープラーニングと従来のプログラミングを融合して、以前では不可能だったアプリケーションを生成するための効果的な方法の実例を示す。  \n",
    "2) ニューラルネットワークアーキテクチャーについて学習するための開始点に導く。  \n",
    "\n",
    "このコースの最初のセクションで、ニューラルネットワークの *学習* と *デプロイ* を正しく行う方法を学びました。\n",
    "従来のプログラミングが画像の分類には不適当であることが判明しましたが、ディープラーニングであれば単に実行できるだけでなく、容易に行えるのです。\n",
    "\n",
    "これまでに、既存のニューラルネットワークとオープンソースデータセット（dogs vs. cats）を取り上げ、あなたの初期のデータセットに似た、ならびに似ていない新しい動物を正しく分類することができるモデルを生成しました。\n",
    "とてもパワフルですね！\n",
    "\n",
    "**この** タスクでは、ディープラーニングにより *画像の分類を超えた* 問題の解き方について、以下のように学びます。\n",
    "\n",
    "- ディープラーニングと従来のコンピュータービジョンを融合する\n",
    "- ニューラルネットワークの内部（実際の数学）を改変する\n",
    "- ジョブにとって適切なネットワークを選定する\n",
    "\n",
    "こうすることで、ディープラーニングに関する第二の課題を解決します。\n",
    "\n",
    "**画像の中でオブジェクトを *検出* し、 *場所を特定* することができるのか？**\n",
    "\n",
    "前のセクションで行ったときと同じ方法で、まずは私たちのモデルのインスタンスを作成してみましょう。\n",
    "モデルのアーキテクチャ、学習済みの重み、および平均画像など、前処理情報を利用するため、モデルとデータセットのジョブディレクトリを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np #Data is often stored as \"Numpy Arrays\"\n",
    "import matplotlib.pyplot as plt #matplotlib.pyplot allows us to visualize results\n",
    "import caffe #caffe is our deep learning framework, we'll learn a lot more about this later in this task.\n",
    "%matplotlib inline\n",
    "\n",
    "MODEL_JOB_DIR = '/dli/data/digits/20180301-185638-e918'  ## Remember to set this to be the job directory for your model\n",
    "DATASET_JOB_DIR = '/dli/data/digits/20180222-165843-ada0'  ## Remember to set this to be the job directory for your dataset\n",
    "\n",
    "MODEL_FILE = MODEL_JOB_DIR + '/deploy.prototxt'                 # This file contains the description of the network architecture\n",
    "PRETRAINED = MODEL_JOB_DIR + '/snapshot_iter_735.caffemodel'    # This file contains the *weights* that were \"learned\" during training\n",
    "MEAN_IMAGE = DATASET_JOB_DIR + '/mean.jpg'                      # This file contains the mean image of the entire dataset. Used to preprocess the data.\n",
    "\n",
    "# Tell Caffe to use the GPU so it can take advantage of parallel processing. \n",
    "# If you have a few hours, you're welcome to change gpu to cpu and see how much time it takes to deploy models in series. \n",
    "caffe.set_mode_gpu()\n",
    "# Initialize the Caffe model using the model trained in DIGITS\n",
    "net = caffe.Classifier(MODEL_FILE, PRETRAINED,\n",
    "                       channel_swap=(2,1,0),\n",
    "                       raw_scale=255,\n",
    "                       image_dims=(256, 256))\n",
    "\n",
    "# load the mean image from the file\n",
    "mean_image = caffe.io.load_image(MEAN_IMAGE)\n",
    "print(\"Ready to predict.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、玄関のセキュリティカメラから画像を直接取得します。\n",
    "この画像の大きさは、256 x 256 を超えていることに注意してください。\n",
    "犬は、この画像の *どこ* にいるのでしょうか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose a random image to test against\n",
    "#RANDOM_IMAGE = str(np.random.randint(10))\n",
    "IMAGE_FILE = '/dli/data/LouieReady.png'\n",
    "input_image= caffe.io.load_image(IMAGE_FILE)\n",
    "plt.imshow(input_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *学習* された関数の利用：順伝播\n",
    "\n",
    "上記の画像サイズの問題は、気をつけるべき事柄です。\n",
    "関数を見てみましょう。\n",
    "\n",
    "<code>prediction = net.predict([grid_square])</code>\n",
    "\n",
    "あらゆる[関数](https://www.khanacademy.org/computing/computer-programming/programming#functions)と同様に、<code>net.predict</code> は入力、<code>grid_square</code> を渡し、出力、<code>prediction</code> を戻します。\n",
    "他の関数とは異なり、この関数はステップの一覧に従わず、層ごとに行列計算を実行して画像を確率ベクトルに変換しています。\n",
    "\n",
    "以下のセルを実行し、無作為で選んだ画像の左上 256 x 256 <code>grid_square</code> からの予測結果を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 0\n",
    "Y = 0\n",
    "\n",
    "grid_square = input_image[X*256:(X+1)*256,Y*256:(Y+1)*256]\n",
    "# subtract the mean image (because we subtracted it while we trained the network - more on this in next lab)\n",
    "grid_square -= mean_image\n",
    "# make prediction\n",
    "prediction = net.predict([grid_square])\n",
    "print prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは、私たちが使用しているネットワークの最後の層の出力です。\n",
    "層の種類は「softmax」で、各セルの値が高ければ高いほど、そのクラスに画像が所属する確率は高くなります。\n",
    "たとえば、ベクトルの第二のセルが第一のセルよりも高い場合（そしてネットワークが学習済みである場合）、画像のそのセクションに *dog* が含まれている可能性が高くなります。\n",
    "今回の場合、第一のセルは *かなり* 高いため、ネットワークが左上の 256 x 256 のグリッドで犬を検出しないのは至極明らかです。\n",
    "\n",
    "オプション: [Explore the math of softmax.](https://en.wikipedia.org/wiki/Softmax_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、画像を繰り返し処理し、それぞれの grid_square を分類してヒートマップを生成するため、私たちの関数の周囲に何らかのコードを実装しましょう。\n",
    "このセクションで主に学ぶことは、この関数の周囲に 「何でも」 構築することができるという事実です。\n",
    "限界があるとすれば、それはあなたの創造力の大きさによって決定付けられます。\n",
    "\n",
    "あなたが持つプログラミング（具体的には Python）の経験に応じて、以下のセルから得る情報は大きく異なる可能性があります。\n",
    "基本的に、このブロックは私たちの入力画像を複数の正方形（256 x 256）に切り、それぞれを犬の分類器で分類し、その判断に従って、犬がいない場所を青色、そして犬がいる場所を赤色とする新しい画像を 1 つ生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the input image into a numpy array and display it\n",
    "input_image = caffe.io.load_image(IMAGE_FILE)\n",
    "plt.imshow(input_image)\n",
    "plt.show()\n",
    "\n",
    "# Calculate how many 256x256 grid squares are in the image\n",
    "rows = input_image.shape[0]/256\n",
    "cols = input_image.shape[1]/256\n",
    "\n",
    "# Initialize an empty array for the detections\n",
    "detections = np.zeros((rows,cols))\n",
    "\n",
    "# Iterate over each grid square using the model to make a class prediction\n",
    "start = time.time()\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        grid_square = input_image[i*256:(i+1)*256,j*256:(j+1)*256]\n",
    "        # subtract the mean image\n",
    "        grid_square -= mean_image\n",
    "        # make prediction\n",
    "        prediction = net.predict([grid_square]) \n",
    "        detections[i,j] = prediction[0].argmax()\n",
    "end = time.time()\n",
    "        \n",
    "# Display the predicted class for each grid square\n",
    "plt.imshow(detections, interpolation=None)\n",
    "\n",
    "# Display total time to perform inference\n",
    "print 'Total inference time: ' + str(end-start) + ' seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "繰り返しになりますが、これはランダムに選ばれた大きなテスト画像に対する出力です。\n",
    "モデルに入力される重複のないそれぞれの 256 x 256 のグリッドに対して、クラスを予測した結果を含む配列と関連しています。\n",
    "\n",
    "どうすればより良く行えたのかを考える前に、以下の問いに答えて今私たちが行ったことについてしばらく考察してみましょう。\n",
    "\n",
    "**物体を検出するため、どのように画像分類器を使用したか、自分の言葉で説明してください。**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "メインコースに戻るときに利用するので、ここにあなたの考えを記録しておいてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このセクションは、あらゆるディープラーニングのソリューションが、入力から出力までのマッピングに過ぎないことを示すことが目的です。\n",
    "これに気づくことで、私たちはより複雑なソリューションを構築する方法への理解に、一歩近づくことになります。\n",
    "たとえば、Alexa、Google Assistant、Siri などの音声アシスタントは、生の音声データをテキストに、テキストを理解に、理解をお気に入りの歌の再生のような求められるタスクに、変換しなければなりません。\n",
    "\n",
    "<a id='question1'></a>\n",
    "\n",
    "複雑さは、コンピューターサイエンスによって構築可能で、これまでにも構築されてきた、到達可能なものです。\n",
    "符号化や問題解決の課題として、このソリューションを構築するときに私たちが考えようとしなかった要因はなんでしょうか？\n",
    "また、私たちはどのようにしてそれらについて説明することができますか？"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ここにあなたの考えを記録してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "想定解答: [ここをクリックしてください](#answer1)\n",
    "\n",
    "### 1.4 オプションのエクササイズに挑戦\n",
    "\n",
    "<a id='question-optional-exercise'></a>\n",
    "\n",
    "1. グリッドの大きさを 256 x 256 に維持して、コードを変換してグリッド間の重複を増やし、より微細な分類マップを取得します。  \n",
    "2. predictionのため、複数のグリッドをまとめて一つのバッチとなるよう、コードを編集します。  \n",
    "\n",
    "解答例: [ここをクリック](#answer-optional-exercise)\n",
    "\n",
    "このように、このスライディングウィンドウアプローチの利点は、（より広く入手することができる）パッチベースの学習データのみを使用して検出器を学習することができ、現在のスキルセットを使用して問題を解決するための作業を開始できることです。\n",
    "\n",
    "私たちはコードの微調整を続け、（不正確で低速な）総当たり方式で問題の解決を続けられるか、確かめることができます。\n",
    "もしくは、私たちはディープラーニングについてより深く学ぶこともできます。\n",
    "後者を選ぶことにしましょう。\n",
    "\n",
    "## アプローチ 2 ー 既存のニューラルネットワークから再構築する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ディープニューラルネットワークは、人の脳をモデルに作られていることを思い出してください。\n",
    "このセクションでは、ネットワークに *変更* を加えて、その挙動とパフォーマンスを変更する方法について解説します。\n",
    "ニューラルネットワークは、根本的には数学的手法によって成り立っており、その手法に変更を加えることで脳に変更が加わります。\n",
    "このセクションでは、各種のネットワークが各種の問題に対して *なぜ* 理想的であるのか、詳しくは扱いません。\n",
    "このことについては、時間の経過とともに直感的に分かるようになるでしょう。\n",
    "代わりに、このセクションでは実験時に留意するべき制約について取り上げます。\n",
    "一般的ないくつかの種類の層について紹介しますが、それらはこのコースで学ぶべき重要な点ではありません。\n",
    "\n",
    "ではこれから、私たちの現在のネットワーク、AlexNet について詳しく探ってゆきましょう。\n",
    "\n",
    "### 2.1 現在のネットワーク\n",
    "\n",
    "ネットワークの構造は、いずれかの *フレームワーク*（今回は Caffe）において記述されます。\n",
    "[数多くのフレームワーク](https://developer.nvidia.com/deep-learning-frameworks) が存在しており、こうしたフレームワークの恩恵により、実務家は抽象的なレベルでネットワークを記述できるのに対して、物理的にはテンソル計算を行うために GPU をプログラムしなければなりません。\n",
    "\n",
    "DIGITS に戻りましょう。\n",
    "\n",
    "## [ここから DIGITS を起動](/digits)\n",
    "\n",
    "AlexNet の Caffe に関する説明を DIGITS で確認するには、以下の手順を行ってください。\n",
    "\n",
    "1) 「Dogs vs. Cats」というモデルを選択します。  \n",
    "2) すべての設定を新しいネットワークにコピーするため、 \"Clone Job\"（右上の青色で示された部分）を選択します。ネットワークのクローンは、実験を繰り返し行うための重要な方法です。  \n",
    "3) AlexNet を選択したモデルの設定の最下部で、 \"Customize\" を選択します。  \n",
    "\n",
    "![](images/customize.png)\n",
    "\n",
    "すると *prototext* が表示されます。\n",
    "これは、AlexNet ネットワークの Caffe の記述です。\n",
    "自由に確認して、どのような構造になっているのか理解してください。\n",
    "他のフレームワークに類似のものが存在しますが、それらの構文は異なっています。\n",
    "\n",
    "次に、ネットワークの最上部で \"Visualize\" を選択して、文字通り AlexNet のネットワークを可視化します。\n",
    "\n",
    "![](images/visualize.png)\n",
    "\n",
    "ここに存在するものは、あなたにとって大きな意味を持たないかも知れませんが、第一に指摘しておくべきことは、prototext に書かれていることが可視化によって表示される内容を直接表現しているということです。\n",
    "確認してみてください。\n",
    "最初の層の *名前* を *train-data* から *your name* に変更し、再び \"*Visualize*\" を選択してください。\n",
    "\n",
    "いかがでしょうか。\n",
    "\n",
    "さて、最初の層の名前を *train-data* に戻し、重要な要素である計算方法を変更しましょう。\n",
    "\n",
    "新しい機能を持てるよう、AlexNet のネットワークのいくつかの層を置き換え、そして、ネットワークがひきつづき学習できるか確認します。\n",
    "\n",
    "### 2.2 脳手術 (のようなこと)\n",
    "\n",
    "1 つの層を置き換えることから始めて、すべてが正しく接続されていることを確かめてください。\n",
    "\n",
    "prototext の中で、「fc6」という名前の層を見つけてください。\n",
    "この層が *何を* 行うのかについて、多くの情報を確認できます。\n",
    "私たちには、いずれ分かります。\n",
    "今のところは、それを置き換えてみましょう。\n",
    "\"layer\" と、次の \"layer\" の前にある閉じる側の[角括弧](#bracket \"}\")の間にあるすべての prototext を、以下の内容で置き換えます。\n",
    "\n",
    "```\n",
    "layer {\n",
    "  name: \"conv6\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"pool5\"\n",
    "  top: \"conv6\"\n",
    "  param {\n",
    "    lr_mult: 1.0\n",
    "    decay_mult: 1.0\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2.0\n",
    "    decay_mult: 0.0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 4096\n",
    "    pad: 0\n",
    "    kernel_size: 6\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "私たちの新しいネットワークと元の AlexNet と比較するため、ネットワークを *可視化* してみましょう。\n",
    "参考までに、オリジナルのネットワークの始点は以下のようになっていました。\n",
    "\n",
    "![](images/AlexNet%20beginning.PNG)\n",
    "\n",
    "そして、終点は以下のとおり。\n",
    "\n",
    "![](images/AlexNetEnd.PNG)\n",
    "\n",
    "これは、私たちが何かを壊したことを示しているでしょうか？"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ここにあなたの考えを記録してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "私たちが壊したものを修正するために、何ができるでしょうか？"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ここにあなたの考えを記録してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ジョブの終了\n",
    "\n",
    "#### ルール 1：データは流れなければならない\n",
    "\n",
    "私たちは、fc6 の層を除去しました。\n",
    "しかし、私たちがネットワークを可視化すると、明らかに除去したはずの層が残っています。\n",
    "これは以下のように *fc6 に言及する他の層が存在している* からです。\n",
    "\n",
    "- fc7\n",
    "- drop6\n",
    "- relu6\n",
    "\n",
    "それらの層が fc6 のどこに接続するように *指示して* いるのか確認するため Caffe の prototext を眺め、代わりに新しい層である <code>conv6</code> に接続します。\n",
    "\n",
    "**再び可視化します。**\n",
    "ネットワークは再び、データが元の AlexNet と同じ入力から出力へ流れることを示すはずです。\n",
    "\n",
    "データが入力から出力へ流れる際、なんらかの操作（計算）を実行する「隠れ層」が数多く存在します。\n",
    "各層の出力は、次の入力として定義される必要があります。\n",
    "これらは大変複合的な機能を生み出します。\n",
    "こうした機能によって、ディープラーニングは入力と出力の間にある、非常に複雑な実世界の関係性をモデル化することが可能となります。\n",
    "\n",
    "その一方で、接続関係を維持する限り、任意の計算を使用することができます。\n",
    "これによりデータは入力から出力へと流れることができます。\n",
    "他の場合を見てみましょう: \n",
    "\n",
    "#### ルール 2：数学的手法の重要性\n",
    "\n",
    "では、私たちが除去した層と追加した層を比較してみましょう。\n",
    "\n",
    "私たちは、従来型の行列の乗算である「全結合 (fully connected)」層を除去しました。\n",
    "行列の乗算については知るべきことがたくさんある一方、私たちが対処するであろうものは、行列の乗算が特定の大きさの行列についてのみ作用するという特性です。\n",
    "私たちの課題に対してこのことが持つ意味合いは、固定されたサイズの画像に入力が制限されているという事実です（私たちの場合、256 x 256）。\n",
    "\n",
    "私たちは「畳み込み (convolutional)」層を追加しましたが、これは入力行列上を移動する「フィルター」機能です。\n",
    "畳み込みについても知るべきことがたくさんある一方、私たちが利用するのは、それらが特定の大きさの行列についてだけ作用するわけではない、という特性です。\n",
    "私たちの課題に対してこのことが持つ意味合いは、すべての全結合層を畳み込み層で置き換えると、あらゆるサイズの画像を受け入れることができるという事実です。\n",
    "\n",
    "**AlexNet を「全畳み込みネットワーク (Fully Convolutional Network)」に変換することで、私たちはさまざまなサイズの画像をグリッドに分割することなく、ネットワークに送り込むことができます。**\n",
    "\n",
    "AlexNet を全畳み込みネットワーク（紛らわしいので「FCN」と表記します）に変換してみましょう。\n",
    "`fc7` から `fc8` までを、以下の Caffe テキストを使用して同等の畳み込み層と置き換えてください。\n",
    "\n",
    "**重要：この置き換えは、「fc8」の定義の末尾において止めるべきです。**\n",
    "\n",
    "```\n",
    "layer {\n",
    "  name: \"conv7\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"conv6\"\n",
    "  top: \"conv7\"\n",
    "  param {\n",
    "    lr_mult: 1.0\n",
    "    decay_mult: 1.0\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2.0\n",
    "    decay_mult: 0.0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 4096\n",
    "    kernel_size: 1\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu7\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"conv7\"\n",
    "  top: \"conv7\"\n",
    "}\n",
    "layer {\n",
    "  name: \"drop7\"\n",
    "  type: \"Dropout\"\n",
    "  bottom: \"conv7\"\n",
    "  top: \"conv7\"\n",
    "  dropout_param {\n",
    "    dropout_ratio: 0.5\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv8\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"conv7\"\n",
    "  top: \"conv8\"\n",
    "  param {\n",
    "    lr_mult: 1.0\n",
    "    decay_mult: 1.0\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2.0\n",
    "    decay_mult: 0.0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    kernel_size: 1\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "引き続きすべてが適切に接続されていることを保証するため、私たちがネットワークに加えなければならない変更は他にありますか？\n",
    "確信が持てないときは、ネットワークを再び *可視化* して評価の助けとしてください。\n",
    "ヒントが必要なときは、[ここ](#rewiring \"`loss` と `accuracy` と `softmax` の各層について、 `bottom` ブロブを `fc8` から `conv8` に変更することをお忘れなく\")にカーソルを合わせてください。\n",
    "\n",
    "すべてが適切に接続されていると確信したら、新しいモデルの学習を始めましょう！\n",
    "\n",
    "![](images/createmodel.PNG)\n",
    "\n",
    "あなたのモデルに名前を付け、*Create* を選択してください。\n",
    "学習の間、以下の動画を見て、あなたがネットワークに加えた畳み込みについて、より深く学んでください。\n",
    "\n",
    "[![What are convolutions?](images/Convolutions.png)](https://youtu.be/BBYnIoGrf8Y?t=13\"What are convolutions.\")\n",
    "\n",
    "エラーなく動作しましたか？すばらしい！\n",
    "もしエラーがあるときは、その内容を確認してください。\n",
    "どこで何かミスを犯したのかが分かります。\n",
    "ありがちなミスは[こちら(カーソルを合わせると詳細が見えます)](#rewiring \"`loss` と `accuracy` と `softmax` の各層について、 `bottom` ブロブを `fc8` から `conv8` に変更することをお忘れなく\")です。\n",
    "答えが欲しいときは、[ここ](#gu)をクリックしてください。\n",
    "\n",
    "<a id='fcn'></a>\n",
    "\n",
    "**モデルの学習が完了したら、最初のセクションで行ったように、以下の ##FIXME## をあなたの新しい「Model Job Directory」で置き換えてください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_DIR = '##FIXME##'  ## Remember to set this to be the job directory for your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自由にコードを確かめて、このモデルを使用することと、そのままの AlexNet を使用することの違いを知ってください。\n",
    "ただし、私たちが学ぶべき大切なことは、グリッドに代えて画像 *全体* を私たちの学習済みモデルに与えることです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import caffe\n",
    "import copy\n",
    "from scipy.misc import imresize\n",
    "import time\n",
    "\n",
    "MODEL_FILE = JOB_DIR + '/deploy.prototxt'                 # Do not change\n",
    "PRETRAINED = JOB_DIR + '/snapshot_iter_735.caffemodel'    # Do not change                 \n",
    "\n",
    "# Tell Caffe to use the GPU\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "# Load the input image into a numpy array and display it\n",
    "input_image = caffe.io.load_image(IMAGE_FILE)\n",
    "plt.imshow(input_image)\n",
    "plt.show()\n",
    "\n",
    "# Initialize the Caffe model using the model trained in DIGITS\n",
    "# This time the model input size is reshaped based on the randomly selected input image\n",
    "net = caffe.Net(MODEL_FILE,PRETRAINED,caffe.TEST)\n",
    "net.blobs['data'].reshape(1, 3, input_image.shape[0], input_image.shape[1])\n",
    "net.reshape()\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "transformer.set_transpose('data', (2,0,1))\n",
    "transformer.set_channel_swap('data', (2,1,0))\n",
    "transformer.set_raw_scale('data', 255.0)\n",
    "\n",
    "# This is just a colormap for displaying the results\n",
    "my_cmap = copy.copy(plt.cm.get_cmap('jet')) # get a copy of the jet color map\n",
    "my_cmap.set_bad(alpha=0) # set how the colormap handles 'bad' values\n",
    "\n",
    "# Feed the whole input image into the model for classification\n",
    "start = time.time()\n",
    "out = net.forward(data=np.asarray([transformer.preprocess('data', input_image)]))\n",
    "end = time.time()\n",
    "\n",
    "# Create an overlay visualization of the classification result\n",
    "im = transformer.deprocess('data', net.blobs['data'].data[0])\n",
    "classifications = out['softmax'][0]\n",
    "classifications = imresize(classifications.argmax(axis=0),input_image.shape,interp='bilinear').astype('float')\n",
    "classifications[classifications==0] = np.nan\n",
    "plt.imshow(im)\n",
    "plt.imshow(classifications,alpha=.5,cmap=my_cmap)\n",
    "plt.show()\n",
    "\n",
    "# Display total time to perform inference\n",
    "print 'Total inference time: ' + str(end-start) + ' seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のコードを複数回実行すると、しばしば、全畳み込みネットワークが、スライディングウィンドウのアプローチよりも高い精度で犬の場所を特定することができるのが分かります。\n",
    "また、FCN のための合計推論時間はおよそ 0.8 秒であった一方、スライディングウィンドウのアプローチでは、その時間は 2.6 秒でした。\n",
    "私たちは、何分の一かの時間でより質の高い結果を得ることができます。\n",
    "たとえば航空機上など、リアルタイムの用途で検出器をデプロイしたい場合、これは大きな利点となります。\n",
    "なぜなら私たちは、一回の効率的なパスで検出と分類を行える単一のモデルを得られるからです。\n",
    "\n",
    "しかし、犬の部分を見落としたり、犬ではないものを誤って検出してしまったりすることがあります。\n",
    "背景にある様々な物体や、犬により引き起こされる誤検出は、適切なデータオーグメンテーションをはじめとする、ニューラルネットワークとは別に行われるデータに関するさまざまな処理を用いて緩和することができます。\n",
    "\n",
    "ネットワークの中で、このような問題をさらに解決することができるでしょうか？\n",
    "\n",
    "シンプルに言えば、Yes です。\n",
    "\n",
    "詳しく説明します。\n",
    "犬を検出し、その場所を特定するよう *設計* された、エンドツーエンドの物体検出ネットワークを使用することで解決できます。\n",
    "私たちは [COCO](http://cocodataset.org/#home) データセットを用いて学習された DetectNet を利用します。\n",
    "作業を進める前に、次のセルを実行してメモリを解放します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del transformer\n",
    "    del net\n",
    "    del detections\n",
    "except Exception as e:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アプローチ 3：DetectNet\n",
    "\n",
    "このラボは、まず AlexNet から始まりました。\n",
    "これは *画像分類* という特定の問題に対する、エレガントな解です。\n",
    "私たちは AlexNet を中心とした Python のプログラムを構築し、小規模な変更を加え、ディープラーニング以前は不可能であったタスクを達成しました。そのタスクは *物体検出* です。\n",
    "\n",
    "しかし、*物体検出* のためのエレガントな解は存在するのでしょうか？\n",
    "私たちは、手元にある入力、すなわちさまざまなサイズの写真から、必要な出力、すなわち位置の特定と検出に関する情報へ、直接的にマッピングするモデルを構築することができるのでしょうか？\n",
    "\n",
    "ある事実が、私たちがディープラーニングの定義を広めるのを後押ししてくれ、ディープラーニングによって解決できる *他の* 問題への洞察をもたらしてくれます。\n",
    "\n",
    "このコースは、ディープラーニングを可能にする 3 つの構成要素についての解説から始まりました。\n",
    "\n",
    "1) ディープニューラルネットワーク  \n",
    "2) GPU  \n",
    "3) ビッグデータ  \n",
    "\n",
    "画像分類を越えて、これらの構成要素は変わりません。\n",
    "しかし、ネットワークの種類、データ、GPU の使用状況は変化します。\n",
    "DIGITS 上のワークフローの構成に即して、私たちはデータから開始します。\n",
    "\n",
    "### データの違い\n",
    "\n",
    "エンドツーエンド教師ありディープラーニングのワークフローを構築するには、ラベル付けされたデータが必要となります。\n",
    "ここまで私たちは、「ラベル付け」を各画像が帰属するカテゴリーの指標として定義していました。\n",
    "しかし、ラベルありデータの実際のタスクは、**入出力のマッピングを作成することです。**\n",
    "\n",
    "この場合、入力を任意のサイズの画像として、出力が画像中にある物体の位置を示すようにしたいと思っています。\n",
    "まず、入力画像は以下のとおりです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = caffe.io.load_image('/dli/data/train/images/488156.jpg') #!ls this directory to see what other images and labels you could test\n",
    "plt.imshow(input_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、それに対応するラベルです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat '/dli/data/train/labels/488156.txt' #\"cat\" has nothing to do with the animals, this displays the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：\n",
    "\n",
    "1) 入力とそれに対応する出力は、ファイル番号に基づいて相互に関連付けられています。  \n",
    "2) あなたが目にしているベクトルは、入力画像内の犬の左上と右下の角の座標 (x,y) で構成されています。  \n",
    "3) サーフボードについて十分なデータを持っているなら、犬の検出器の代わりにサーフボードの検出器を学習することができます。私たちは、犬を探すことのみを目的にデータセットを設定しています。  \n",
    "\n",
    "このデータセットを DIGITS に読み込みましょう。\n",
    "\n",
    "[DIGITS](/digits) のホーム画面から、\"Datasets\" を選択し、新しい **\"Object Detection\" データセット** を加えます。\n",
    "パラメーターは、分類のときとは異なって見えるのでご注意ください。\n",
    "\n",
    "\"New Object Detection Dataset\" パネルが開いたら、以下の画像から前処理オプションを選んでください（**前後の余白にご注意ください**）：\n",
    "\n",
    "Training image folder: /dli/data/train/images  \n",
    "Training label folder: /dli/data/train/labels  \n",
    "Validation image folder: /dli/data/val/images  \n",
    "Validation label folder: /dli/data/val/labels/dog  \n",
    "Pad image (Width x Height): 640 x 640  \n",
    "Custom classes: dontcare, dog  \n",
    "Group Name: MS-COCO  \n",
    "Dataset Name: coco-dog  \n",
    "\n",
    "![OD data ingest](images/OD_ingest.png)\n",
    "\n",
    "注：データセットが読み込みを完了するのを待つことなく、モデルの学習ジョブの設定を開始しても構いません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ネットワークの違い\n",
    "\n",
    "さまざまな種類のことがらを *学習* するために設計された、さまざまな種類の人工「頭脳」が存在します。\n",
    "あなたの課題と類似している問題を解決するためにどのようなアーキテクチャが用いられているか学ぶことは、ディープラーニングを成功させるための核となるスキルの一つです。\n",
    "アプローチ 2 では、いくつかの制約を変更するだけで、ネットワークの挙動を変えてしまう可能性があることを目にしました。\n",
    "アプローチ 3 では、設計者でなくてもこれらのネットワークとモデルを使用することができることに注意しつつ、最新の研究や専門知識、および繰り返し計算することがもたらすメリットを示します。\n",
    "私たちが利用するネットワークの設計者が直面した課題とは、**データから入出力のペアをマッピングするための最も効率的で正確な方法とは何か** ということでした。\n",
    "\n",
    "ネットワークを読み込み、可視化することで、これらのネットワークがいかに多様性に満ちたものであるか把握してみましょう。\n",
    "\n",
    "画面左上で \"DIGITS\" を選択して、DIGITS のホーム画面に戻ってください。\n",
    "\"Image\" -> \"Object Detection\" の順に選択して、新しい Object Detection **モデル** を作成してください。\n",
    "\n",
    "あなたがさきほど読み込んだデータセット、\"coco-dog\" を選択してください。\n",
    "\n",
    "以下の理由により、エポック数を 1 に変更し、学習率を 1e-100 に変更してください。\n",
    "通常、ネットワークを選択するときは、\"Custom Network\"を選択します。\n",
    "空のテキストエリアが開くはずです。\n",
    "\n",
    "私たちは、\"DetectNet deploy.prototxt\" と検索することによって、このネットワークの定義を見つけることができます。\n",
    "あなたがやるべきことは、このリンク：[deploy.prototxt](../../../../edit/tasks/task5/task/deploy.prototxt) からのテキストを、表示されたフィールドにコピーして貼り付けることです。\n",
    "このとき、ネットワークのアーキテクチャーを共有することがこれほどまでに容易であることに留意してください。\n",
    "\n",
    "次に、\"visualize\" を選択してネットワークを可視化してください。\n",
    "このネットワークの大きさと複雑さが、 AlexNet のそれとは異なることに注意してください。\n",
    "DetectNet は実際には、私たちが上で構築したような全畳み込みネットワーク（FCN）ですが、このデータ表現をその出力として正確に生成するように構成されています。\n",
    "DetectNet の多くの層は、よく知られた GoogLeNet ネットワークと同一です。\n",
    "\n",
    "まだ学習を始めないでください！\n",
    "\n",
    "### 計算の違い\n",
    "\n",
    "ネットワークの深さとタスクの複雑さが高まると、これまで作業してきたネットワークと比較してより *多くの学習* が必要となります。\n",
    "このワークフローの威力を示すために使用するモデルは、 NVIDIA Tesla K80 上で学習するのにおよそ 16 時間かかりました（Volta ではこれよりも短く、CPU では人生を捧げることになります）。\n",
    "\n",
    "このため、最初から学習を行う代わりに、事前学習済みのモデルから始め、とても遅い（小さい）学習率（前のステップですでに設定しているもの）で 1 エポックだけ学習します。\n",
    "\n",
    "学習済みの重みを読み込むには、重みを <pre>/dli/data/digits/snapshot_iter_38600.caffemodel</pre> のファイルと共に <code>Pretrained Model(s)</code> のフィールドに読み込みます。\n",
    "\n",
    "[](images/loadsnapshot.PNG)\n",
    "\n",
    "自分のモデルに名前を付け、それを「学習」して、画像分類と物体検出の違いを確認します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの学習を行うと、画像分類モデルのために報告されたもとのとは異なるパフォーマンスの測定値が確認できます。\n",
    "平均精度（mAP）とは、ネットワークがどれほど良好に犬を検出することができるか、ならびにそのバウンディングボックス（位置特定）の推定が検証データセットに対してどれほど正確かに関する複合的な測定値です。\n",
    "\n",
    "100 エポック後に、このモデルが低い学習/検証損失の値に収束したことだけでなく、高い mAP スコアを確認できます。\n",
    "この学習済みモデルをテスト画像と比較してテストし、犬を見つけることができるか確認してみましょう。\n",
    "\n",
    "**可視化方法(\"Select Visualization Method\")を \"Bounding Boxes\" に設定** し、画像パス `/dli/data/BeagleImages/LouieTest1.jpg` を \"Image Path\" にペーストし、\"Test One\" をクリックしてください。\n",
    "DetectNet が Louie を検出し、バウンディングボックスを描く様子が確認できるはずです。\n",
    "\n",
    "試すべき他の画像の一覧にアクセスするには、以下のセルを実行してください。\n",
    "Jupyter ノートブックの \"!\" は、こうした各セルをコマンドラインと同様に振る舞わせることにご注意ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /dli/data/BeagleImages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dli/data/BeagleImages` フォルダーからの他の画像を自由にテストしてください。\n",
    "あなたは、DetectNet が、厳密に描かれたバウンディングボックスで多くの犬を正確に検出することができ、誤報率がとても低いことがわかるでしょう。\n",
    "このモデルは、複数のクラスの物体を検出することができるという点も、注目に値します！\n",
    "\n",
    "ここで学ぶべきことがいくつかあります。\n",
    "\n",
    "1. 入手可能ななんらかのジョブ向けの適切なネットワークとデータは、独自のソリューションをハックするより相当に優れています。\n",
    "2. 適切なネットワーク（および、時には事前学習済みモデル）は、DIGITS または利用しているフレームワークの [Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo) から使用できる可能性があります。\n",
    "\n",
    "このため、利用可能な1つの問題解決フレームワークは、\n",
    "\n",
    "- 誰かがすでに、あなたの課題を解決しているかどうか判断した場合、そのモデルを使用します。\n",
    "- まだ誰もあなたの課題そのものを解決していない場合で、誰かがすでに、あなたの課題に類似した課題を解決したと判断した場合、彼らのネットワークと手持ちのデータを使用します。\n",
    "- 誰もあなたの課題自体や類似の課題を解決しておらず、あなたの課題を解決するための誰かの解決策の欠点を特定し、新しいネットワークの設計に取り組むことができるか判断します。\n",
    "- それを行えない場合、既存の解決策、その他の課題解決の技術（Pythonなど）を使用して手持ちの課題を解決します。\n",
    "- いずれにしても、実験を続け、ラボを利用してスキルを向上させてください！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に進む前に、これまでに学んだことを振り返ってみましょう。\n",
    "\n",
    "すでに以下のことを習得済みです。\n",
    "\n",
    "- DIGITS を使用してさまざまな種類のニューラルネットワークを学習する方法。\n",
    "- どのような種類の変更が、パフォーマンスを向上させるために、ネットワークを学習する方法に加えることができるか、ということ。\n",
    "- 新しい課題を解決するために、ディープラーニングと従来的なプログラムを組み合わせる方法。\n",
    "- 機能と制約を変更するために、ネットワーク内部の層を変更する方法。\n",
    "- あなたの課題に類似した課題に取り組んだ人たちを確認し、彼らのソリューションのどの部分をあなたが利用できるのかを調べること。\n",
    "\n",
    "次に、あなたが取り組み始めている課題を解決するために、ディープニューラルネットワークを本番環境に *デプロイ* する方法について、さらに深く学びましょう。\n",
    "あなたが必要とするであろう情報やデプロイ時のパフォーマンスに関する基準、およびエンドユーザーが管理しているディープラーニングの力と速さを維持する（もしくは、高める）ツールについて、さらに理解を深めましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題の解答：\n",
    "\n",
    "<a id='answer1'></a>\n",
    "### 解答1\n",
    "\n",
    "私たちが重複しないグリッドのスライディングウィンドウを使用したのは、私たちのグリッドのいくつかには犬が部分的にしか含まれておらず、分類の失敗につながる可能性があるからです。\n",
    "しかし、グリッドにおいて重複する部分を増やしてゆくと、このスライディングウィンドウのアプローチのための計算時間が急速に増えることになります。\n",
    "私たちは、入力を *バッチ化* することで、計算におけるこの上昇を軽減することができます。\n",
    "これは、GPU が持つ並行処理の本質的な力を利用した戦略です。\n",
    "\n",
    "[ここをクリック](#question1)して、ラボに戻ってください。\n",
    "\n",
    "<a id='answer-optional-exercise'></a>\n",
    "\n",
    "### オプション課題への解答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import caffe\n",
    "import time\n",
    "\n",
    "MODEL_JOB_NUM = '##FIXME##'  ## Remember to set this to be the job number for your model\n",
    "DATASET_JOB_NUM = '##FIXME##'  ## Remember to set this to be the job number for your dataset\n",
    "\n",
    "MODEL_FILE = MODEL_JOB_NUM + '/deploy.prototxt'                 # Do not change\n",
    "PRETRAINED = MODEL_JOB_NUM + '/snapshot_iter_735.caffemodel'    # Do not change\n",
    "MEAN_IMAGE = DATASET_JOB_NUM + '/mean.jpg'                      # Do not change\n",
    "\n",
    "# load the mean image\n",
    "mean_image = caffe.io.load_image(MEAN_IMAGE)\n",
    "\n",
    "# Choose a random image to test against\n",
    "#RANDOM_IMAGE = str(np.random.randint(10))\n",
    "IMAGE_FILE = '/dli/data/LouieReady.png' \n",
    "\n",
    "# Tell Caffe to use the GPU\n",
    "caffe.set_mode_gpu()\n",
    "# Initialize the Caffe model using the model trained in DIGITS\n",
    "net = caffe.Classifier(MODEL_FILE, PRETRAINED,\n",
    "                       channel_swap=(2,1,0),\n",
    "                       raw_scale=255,\n",
    "                       image_dims=(256, 256))\n",
    "\n",
    "# Load the input image into a numpy array and display it\n",
    "input_image = caffe.io.load_image(IMAGE_FILE)\n",
    "plt.imshow(input_image)\n",
    "plt.show()\n",
    "\n",
    "# Calculate how many 256x256 grid squares are in the image\n",
    "rows = input_image.shape[0]/256\n",
    "cols = input_image.shape[1]/256\n",
    "\n",
    "# Subtract the mean image\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        input_image[i*256:(i+1)*256,j*256:(j+1)*256] -= mean_image\n",
    "        \n",
    "# Initialize an empty array for the detections\n",
    "detections = np.zeros((rows,cols))\n",
    "        \n",
    "# Iterate over each grid square using the model to make a class prediction\n",
    "start = time.time()\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        grid_square = input_image[i*256:(i+1)*256,j*256:(j+1)*256]\n",
    "        # make prediction\n",
    "        prediction = net.predict([grid_square])\n",
    "        detections[i,j] = prediction[0].argmax()\n",
    "end = time.time()\n",
    "        \n",
    "# Display the predicted class for each grid square\n",
    "plt.imshow(detections)\n",
    "plt.show()\n",
    "\n",
    "# Display total time to perform inference\n",
    "print 'Total inference time (sliding window without overlap): ' + str(end-start) + ' seconds'\n",
    "\n",
    "# define the amount of overlap between grid cells\n",
    "OVERLAP = 0.25\n",
    "grid_rows = int((rows-1)/(1-OVERLAP))+1\n",
    "grid_cols = int((cols-1)/(1-OVERLAP))+1\n",
    "\n",
    "print \"Image has %d*%d blocks of 256 pixels\" % (rows, cols)\n",
    "print \"With overlap=%f grid_size=%d*%d\" % (OVERLAP, grid_rows, grid_cols)\n",
    "\n",
    "# Initialize an empty array for the detections\n",
    "detections = np.zeros((grid_rows,grid_cols))\n",
    "\n",
    "# Iterate over each grid square using the model to make a class prediction\n",
    "start = time.time()\n",
    "for i in range(0,grid_rows):\n",
    "    for j in range(0,grid_cols):\n",
    "        start_col = int(j*256*(1-OVERLAP))\n",
    "        start_row = int(i*256*(1-OVERLAP))\n",
    "        grid_square = input_image[start_row:start_row+256, start_col:start_col+256]\n",
    "        # make prediction\n",
    "        prediction = net.predict([grid_square])\n",
    "        detections[i,j] = prediction[0].argmax()\n",
    "end = time.time()\n",
    "        \n",
    "# Display the predicted class for each grid square\n",
    "plt.imshow(detections)\n",
    "plt.show()\n",
    "\n",
    "# Display total time to perform inference\n",
    "print ('Total inference time (sliding window with %f%% overlap: ' % (OVERLAP*100)) + str(end-start) + ' seconds'\n",
    "\n",
    "# now with batched inference (one column at a time)\n",
    "# we are not using a caffe.Classifier here so we need to do the pre-processing\n",
    "# manually. The model was trained on random crops (256*256->227*227) so we\n",
    "# need to do the cropping below. Similarly, we need to convert images\n",
    "# from Numpy's Height*Width*Channel (HWC) format to Channel*Height*Width (CHW) \n",
    "# Lastly, we need to swap channels from RGB to BGR\n",
    "net = caffe.Net(MODEL_FILE, PRETRAINED, caffe.TEST)\n",
    "start = time.time()\n",
    "net.blobs['data'].reshape(*[grid_cols, 3, 227, 227])\n",
    "\n",
    "# Initialize an empty array for the detections\n",
    "detections = np.zeros((rows,cols))\n",
    "\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        grid_square = input_image[i*256:(i+1)*256,j*256:(j+1)*256]\n",
    "        # add to batch\n",
    "        grid_square = grid_square[14:241,14:241] # 227*227 center crop        \n",
    "        image = np.copy(grid_square.transpose(2,0,1)) # transpose from HWC to CHW\n",
    "        image = image * 255 # rescale\n",
    "        image = image[(2,1,0), :, :] # swap channels\n",
    "        net.blobs['data'].data[j] = image\n",
    "    # make prediction\n",
    "    output = net.forward()[net.outputs[-1]]\n",
    "    for j in range(0,cols):\n",
    "        detections[i,j] = output[j].argmax()\n",
    "end = time.time()\n",
    "        \n",
    "# Display the predicted class for each grid square\n",
    "plt.imshow(detections)\n",
    "plt.show()\n",
    "\n",
    "# Display total time to perform inference\n",
    "print 'Total inference time (batched inference): ' + str(end-start) + ' seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完了したら、[ラボに戻ってください。](#question-optional-exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gu'></a>\n",
    "# 全畳み込みネットワークの解答\n",
    "\n",
    "AlexNet 内の **すべて** のテキストを以下と置き換えて、それを全畳み込みネットワークに変換することができます。\n",
    "コピーしたら、[ここ](#fcn) をクリックしてラボに戻ってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```# AlexNet\n",
    "name: \"AlexNet\"\n",
    "layer {\n",
    "  name: \"train-data\"\n",
    "  type: \"Data\"\n",
    "  top: \"data\"\n",
    "  top: \"label\"\n",
    "  transform_param {\n",
    "    mirror: true\n",
    "    crop_size: 227\n",
    "  }\n",
    "  data_param {\n",
    "    batch_size: 128\n",
    "  }\n",
    "  include { stage: \"train\" }\n",
    "}\n",
    "layer {\n",
    "  name: \"val-data\"\n",
    "  type: \"Data\"\n",
    "  top: \"data\"\n",
    "  top: \"label\"\n",
    "  transform_param {\n",
    "    crop_size: 227\n",
    "  }\n",
    "  data_param {\n",
    "    batch_size: 32\n",
    "  }\n",
    "  include { stage: \"val\" }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv1\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"data\"\n",
    "  top: \"conv1\"\n",
    "  param {\n",
    "    lr_mult: 1\n",
    "    decay_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2\n",
    "    decay_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 96\n",
    "    kernel_size: 11\n",
    "    stride: 4\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu1\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"conv1\"\n",
    "  top: \"conv1\"\n",
    "}\n",
    "layer {\n",
    "  name: \"norm1\"\n",
    "  type: \"LRN\"\n",
    "  bottom: \"conv1\"\n",
    "  top: \"norm1\"\n",
    "  lrn_param {\n",
    "    local_size: 5\n",
    "    alpha: 0.0001\n",
    "    beta: 0.75\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool1\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"norm1\"\n",
    "  top: \"pool1\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 3\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv2\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"pool1\"\n",
    "  top: \"conv2\"\n",
    "  param {\n",
    "    lr_mult: 1\n",
    "    decay_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2\n",
    "    decay_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 256\n",
    "    pad: 2\n",
    "    kernel_size: 5\n",
    "    group: 2\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu2\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"conv2\"\n",
    "  top: \"conv2\"\n",
    "}\n",
    "layer {\n",
    "  name: \"norm2\"\n",
    "  type: \"LRN\"\n",
    "  bottom: \"conv2\"\n",
    "  top: \"norm2\"\n",
    "  lrn_param {\n",
    "    local_size: 5\n",
    "    alpha: 0.0001\n",
    "    beta: 0.75\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"pool2\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"norm2\"\n",
    "  top: \"pool2\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 3\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv3\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"pool2\"\n",
    "  top: \"conv3\"\n",
    "  param {\n",
    "    lr_mult: 1\n",
    "    decay_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2\n",
    "    decay_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 384\n",
    "    pad: 1\n",
    "    kernel_size: 3\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu3\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"conv3\"\n",
    "  top: \"conv3\"\n",
    "}\n",
    "layer {\n",
    "  name: \"conv4\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"conv3\"\n",
    "  top: \"conv4\"\n",
    "  param {\n",
    "    lr_mult: 1\n",
    "    decay_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2\n",
    "    decay_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 384\n",
    "    pad: 1\n",
    "    kernel_size: 3\n",
    "    group: 2\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu4\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"conv4\"\n",
    "  top: \"conv4\"\n",
    "}\n",
    "layer {\n",
    "  name: \"conv5\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"conv4\"\n",
    "  top: \"conv5\"\n",
    "  param {\n",
    "    lr_mult: 1\n",
    "    decay_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2\n",
    "    decay_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 256\n",
    "    pad: 1\n",
    "    kernel_size: 3\n",
    "    group: 2\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu5\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"conv5\"\n",
    "  top: \"conv5\"\n",
    "}\n",
    "layer {\n",
    "  name: \"pool5\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv5\"\n",
    "  top: \"pool5\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 3\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv6\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"pool5\"\n",
    "  top: \"conv6\"\n",
    "  param {\n",
    "    lr_mult: 1.0\n",
    "    decay_mult: 1.0\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2.0\n",
    "    decay_mult: 0.0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 4096\n",
    "    pad: 0\n",
    "    kernel_size: 6\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu6\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"conv6\"\n",
    "  top: \"conv6\"\n",
    "}\n",
    "layer {\n",
    "  name: \"drop6\"\n",
    "  type: \"Dropout\"\n",
    "  bottom: \"conv6\"\n",
    "  top: \"conv6\"\n",
    "  dropout_param {\n",
    "    dropout_ratio: 0.5\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv7\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"conv6\"\n",
    "  top: \"conv7\"\n",
    "  param {\n",
    "    lr_mult: 1.0\n",
    "    decay_mult: 1.0\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2.0\n",
    "    decay_mult: 0.0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 4096\n",
    "    kernel_size: 1\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu7\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"conv7\"\n",
    "  top: \"conv7\"\n",
    "}\n",
    "layer {\n",
    "  name: \"drop7\"\n",
    "  type: \"Dropout\"\n",
    "  bottom: \"conv7\"\n",
    "  top: \"conv7\"\n",
    "  dropout_param {\n",
    "    dropout_ratio: 0.5\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv8\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"conv7\"\n",
    "  top: \"conv8\"\n",
    "  param {\n",
    "    lr_mult: 1.0\n",
    "    decay_mult: 1.0\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2.0\n",
    "    decay_mult: 0.0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    kernel_size: 1\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"accuracy\"\n",
    "  type: \"Accuracy\"\n",
    "  bottom: \"conv8\"\n",
    "  bottom: \"label\"\n",
    "  top: \"accuracy\"\n",
    "  include { stage: \"val\" }\n",
    "}\n",
    "layer {\n",
    "  name: \"loss\"\n",
    "  type: \"SoftmaxWithLoss\"\n",
    "  bottom: \"conv8\"\n",
    "  bottom: \"label\"\n",
    "  top: \"loss\"\n",
    "  exclude { stage: \"deploy\" }\n",
    "}\n",
    "layer {\n",
    "  name: \"softmax\"\n",
    "  type: \"Softmax\"\n",
    "  bottom: \"conv8\"\n",
    "  top: \"softmax\"\n",
    "  include { stage: \"deploy\" }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コピー後、[ここ](#fcn) をクリックしてラボに戻ってください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
