{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-atf3gekcgR"
   },
   "source": [
    "# 评估 1：我可以训练并部署神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7wkT17FkmU6"
   },
   "source": [
    "此时，您已学习完深度学习的全部工作流程。您已经完成数据集加载、模型训练，以及将模型部署到简单的应用程序中。接下来，您可尝试重复该工作流程来解决新问题，以此验证学习效果。\n",
    "\n",
    "我们提供了由以下两类图像组成的数据集：\n",
    "\n",
    "1) 面部：包括含有鲸鱼面部的图像\n",
    "2) 非面部：包括不含有鲸鱼面部的图像。\n",
    "\n",
    "数据集位于 ```/dli/data/whale/data/train```。\n",
    "\n",
    "您要完成的挑战是：\n",
    "\n",
    "1) 使用 [DIGITS](/digits) 训练模型，以识别*新的*鲸鱼面部图像，准确度需超过 80%。\n",
    "\n",
    "2) 通过修改和保存 python 应用程序 [submission.py](../../../../edit/tasks/task-assessment/task/submission.py) 来部署模型，使之在图像包含鲸鱼面部时返回词“whale”（鲸），否则便返回“not whale”（非鲸）。\n",
    "\n",
    "资源：\n",
    "\n",
    "1) [训练模型](../../task1/task/Train%20a%20Model.ipynb)  \n",
    "2) [目标数据](../../task2/task/New%20Data%20as%20a%20Goal.ipynb)  \n",
    "3) [部署](../../task3/task/Deployment.ipynb)  \n",
    "\n",
    "建议： \n",
    "\n",
    "- 使用空代码块查找所有必要信息，以解决此问题，例如：```!ls [directorypath] prints the files in a given directory```\n",
    "- 执行下方前两个单元将运行python脚本对测试图片进行预测, 且第一个单元应返回 “whale” （鲸），第二个单元应返回 “not whale” （非鲸）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YaaY1Vb3o3mC"
   },
   "source": [
    "在 [DIGITS](/digits/) 中启动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0618 07:43:35.206017   174 gpu_memory.cpp:105] GPUMemory::Manager initialized\n",
      "I0618 07:43:35.206775   174 gpu_memory.cpp:107] Total memory: 11996954624, Free: 11791564800, dev_info[0]: total=11996954624 free=11791564800\n",
      "W0618 07:43:35.206838   174 _caffe.cpp:172] DEPRECATION WARNING - deprecated use of Python interface\n",
      "W0618 07:43:35.206952   174 _caffe.cpp:173] Use this instead (with the named \"weights\" parameter):\n",
      "W0618 07:43:35.206969   174 _caffe.cpp:175] Net('/dli/data/digits/20190618-072806-d33f/deploy.prototxt', 1, weights='/dli/data/digits/20190618-072806-d33f/snapshot_iter_1620.caffemodel')\n",
      "I0618 07:43:35.207326   174 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: /dli/data/digits/20190618-072806-d33f/deploy.prototxt\n",
      "I0618 07:43:35.207367   174 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.\n",
      "W0618 07:43:35.207386   174 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.\n",
      "I0618 07:43:35.216569   174 net.cpp:79] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"input\"\n",
      "  type: \"Input\"\n",
      "  top: \"data\"\n",
      "  input_param {\n",
      "    shape {\n",
      "      dim: 1\n",
      "      dim: 3\n",
      "      dim: 227\n",
      "      dim: 227\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 96\n",
      "    kernel_size: 11\n",
      "    stride: 4\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"conv1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv4\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv4\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu4\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv4\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv5\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv5\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu5\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"conv5\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool5\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"pool5\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc6\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool5\"\n",
      "  top: \"fc6\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu6\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop6\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc7\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc7\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu7\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop7\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc8\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc8\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"softmax\"\n",
      "  type: \"Softmax\"\n",
      "  bottom: \"fc8\"\n",
      "  top: \"softmax\"\n",
      "}\n",
      "I0618 07:43:35.217351   174 net.cpp:109] Using FLOAT as default forward math type\n",
      "I0618 07:43:35.217372   174 net.cpp:115] Using FLOAT as default backward math type\n",
      "I0618 07:43:35.217392   174 layer_factory.hpp:172] Creating layer 'input' of type 'Input'\n",
      "I0618 07:43:35.217408   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.217432   174 net.cpp:199] Created Layer input (0)\n",
      "I0618 07:43:35.217450   174 net.cpp:541] input -> data\n",
      "I0618 07:43:35.218140   174 net.cpp:259] Setting up input\n",
      "I0618 07:43:35.218176   174 net.cpp:266] TEST Top shape for layer 0 'input' 1 3 227 227 (154587)\n",
      "I0618 07:43:35.218197   174 layer_factory.hpp:172] Creating layer 'conv1' of type 'Convolution'\n",
      "I0618 07:43:35.218214   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.218252   174 net.cpp:199] Created Layer conv1 (1)\n",
      "I0618 07:43:35.218294   174 net.cpp:571] conv1 <- data\n",
      "I0618 07:43:35.218314   174 net.cpp:541] conv1 -> conv1\n",
      "I0618 07:43:35.729820   174 net.cpp:259] Setting up conv1\n",
      "I0618 07:43:35.729874   174 net.cpp:266] TEST Top shape for layer 1 'conv1' 1 96 55 55 (290400)\n",
      "I0618 07:43:35.729916   174 layer_factory.hpp:172] Creating layer 'relu1' of type 'ReLU'\n",
      "I0618 07:43:35.729935   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.729950   174 net.cpp:199] Created Layer relu1 (2)\n",
      "I0618 07:43:35.729964   174 net.cpp:571] relu1 <- conv1\n",
      "I0618 07:43:35.729972   174 net.cpp:526] relu1 -> conv1 (in-place)\n",
      "I0618 07:43:35.729998   174 net.cpp:259] Setting up relu1\n",
      "I0618 07:43:35.730011   174 net.cpp:266] TEST Top shape for layer 2 'relu1' 1 96 55 55 (290400)\n",
      "I0618 07:43:35.730026   174 layer_factory.hpp:172] Creating layer 'norm1' of type 'LRN'\n",
      "I0618 07:43:35.730041   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.730072   174 net.cpp:199] Created Layer norm1 (3)\n",
      "I0618 07:43:35.730084   174 net.cpp:571] norm1 <- conv1\n",
      "I0618 07:43:35.730090   174 net.cpp:541] norm1 -> norm1\n",
      "I0618 07:43:35.730154   174 net.cpp:259] Setting up norm1\n",
      "I0618 07:43:35.730172   174 net.cpp:266] TEST Top shape for layer 3 'norm1' 1 96 55 55 (290400)\n",
      "I0618 07:43:35.730180   174 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'\n",
      "I0618 07:43:35.730186   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.730229   174 net.cpp:199] Created Layer pool1 (4)\n",
      "I0618 07:43:35.730238   174 net.cpp:571] pool1 <- norm1\n",
      "I0618 07:43:35.730247   174 net.cpp:541] pool1 -> pool1\n",
      "I0618 07:43:35.730336   174 net.cpp:259] Setting up pool1\n",
      "I0618 07:43:35.730353   174 net.cpp:266] TEST Top shape for layer 4 'pool1' 1 96 27 27 (69984)\n",
      "I0618 07:43:35.730366   174 layer_factory.hpp:172] Creating layer 'conv2' of type 'Convolution'\n",
      "I0618 07:43:35.730373   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.730398   174 net.cpp:199] Created Layer conv2 (5)\n",
      "I0618 07:43:35.730409   174 net.cpp:571] conv2 <- pool1\n",
      "I0618 07:43:35.730417   174 net.cpp:541] conv2 -> conv2\n",
      "I0618 07:43:35.737360   174 net.cpp:259] Setting up conv2\n",
      "I0618 07:43:35.737387   174 net.cpp:266] TEST Top shape for layer 5 'conv2' 1 256 27 27 (186624)\n",
      "I0618 07:43:35.737411   174 layer_factory.hpp:172] Creating layer 'relu2' of type 'ReLU'\n",
      "I0618 07:43:35.737424   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.737440   174 net.cpp:199] Created Layer relu2 (6)\n",
      "I0618 07:43:35.737457   174 net.cpp:571] relu2 <- conv2\n",
      "I0618 07:43:35.737473   174 net.cpp:526] relu2 -> conv2 (in-place)\n",
      "I0618 07:43:35.737490   174 net.cpp:259] Setting up relu2\n",
      "I0618 07:43:35.737505   174 net.cpp:266] TEST Top shape for layer 6 'relu2' 1 256 27 27 (186624)\n",
      "I0618 07:43:35.737517   174 layer_factory.hpp:172] Creating layer 'norm2' of type 'LRN'\n",
      "I0618 07:43:35.737529   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.737548   174 net.cpp:199] Created Layer norm2 (7)\n",
      "I0618 07:43:35.737558   174 net.cpp:571] norm2 <- conv2\n",
      "I0618 07:43:35.737571   174 net.cpp:541] norm2 -> norm2\n",
      "I0618 07:43:35.737629   174 net.cpp:259] Setting up norm2\n",
      "I0618 07:43:35.737645   174 net.cpp:266] TEST Top shape for layer 7 'norm2' 1 256 27 27 (186624)\n",
      "I0618 07:43:35.737659   174 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'\n",
      "I0618 07:43:35.737665   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.737684   174 net.cpp:199] Created Layer pool2 (8)\n",
      "I0618 07:43:35.737696   174 net.cpp:571] pool2 <- norm2\n",
      "I0618 07:43:35.737709   174 net.cpp:541] pool2 -> pool2\n",
      "I0618 07:43:35.737773   174 net.cpp:259] Setting up pool2\n",
      "I0618 07:43:35.737788   174 net.cpp:266] TEST Top shape for layer 8 'pool2' 1 256 13 13 (43264)\n",
      "I0618 07:43:35.737800   174 layer_factory.hpp:172] Creating layer 'conv3' of type 'Convolution'\n",
      "I0618 07:43:35.737812   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.737831   174 net.cpp:199] Created Layer conv3 (9)\n",
      "I0618 07:43:35.737843   174 net.cpp:571] conv3 <- pool2\n",
      "I0618 07:43:35.737854   174 net.cpp:541] conv3 -> conv3\n",
      "I0618 07:43:35.753371   174 net.cpp:259] Setting up conv3\n",
      "I0618 07:43:35.753396   174 net.cpp:266] TEST Top shape for layer 9 'conv3' 1 384 13 13 (64896)\n",
      "I0618 07:43:35.753413   174 layer_factory.hpp:172] Creating layer 'relu3' of type 'ReLU'\n",
      "I0618 07:43:35.753424   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.753432   174 net.cpp:199] Created Layer relu3 (10)\n",
      "I0618 07:43:35.753445   174 net.cpp:571] relu3 <- conv3\n",
      "I0618 07:43:35.753458   174 net.cpp:526] relu3 -> conv3 (in-place)\n",
      "I0618 07:43:35.753473   174 net.cpp:259] Setting up relu3\n",
      "I0618 07:43:35.753490   174 net.cpp:266] TEST Top shape for layer 10 'relu3' 1 384 13 13 (64896)\n",
      "I0618 07:43:35.753497   174 layer_factory.hpp:172] Creating layer 'conv4' of type 'Convolution'\n",
      "I0618 07:43:35.753509   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.753533   174 net.cpp:199] Created Layer conv4 (11)\n",
      "I0618 07:43:35.753545   174 net.cpp:571] conv4 <- conv3\n",
      "I0618 07:43:35.753551   174 net.cpp:541] conv4 -> conv4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0618 07:43:35.767127   174 net.cpp:259] Setting up conv4\n",
      "I0618 07:43:35.767156   174 net.cpp:266] TEST Top shape for layer 11 'conv4' 1 384 13 13 (64896)\n",
      "I0618 07:43:35.767195   174 layer_factory.hpp:172] Creating layer 'relu4' of type 'ReLU'\n",
      "I0618 07:43:35.767208   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.767222   174 net.cpp:199] Created Layer relu4 (12)\n",
      "I0618 07:43:35.767235   174 net.cpp:571] relu4 <- conv4\n",
      "I0618 07:43:35.767246   174 net.cpp:526] relu4 -> conv4 (in-place)\n",
      "I0618 07:43:35.767261   174 net.cpp:259] Setting up relu4\n",
      "I0618 07:43:35.767273   174 net.cpp:266] TEST Top shape for layer 12 'relu4' 1 384 13 13 (64896)\n",
      "I0618 07:43:35.767285   174 layer_factory.hpp:172] Creating layer 'conv5' of type 'Convolution'\n",
      "I0618 07:43:35.767295   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.767320   174 net.cpp:199] Created Layer conv5 (13)\n",
      "I0618 07:43:35.767331   174 net.cpp:571] conv5 <- conv4\n",
      "I0618 07:43:35.767343   174 net.cpp:541] conv5 -> conv5\n",
      "I0618 07:43:35.775368   174 net.cpp:259] Setting up conv5\n",
      "I0618 07:43:35.775396   174 net.cpp:266] TEST Top shape for layer 13 'conv5' 1 256 13 13 (43264)\n",
      "I0618 07:43:35.775420   174 layer_factory.hpp:172] Creating layer 'relu5' of type 'ReLU'\n",
      "I0618 07:43:35.775435   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.775452   174 net.cpp:199] Created Layer relu5 (14)\n",
      "I0618 07:43:35.775468   174 net.cpp:571] relu5 <- conv5\n",
      "I0618 07:43:35.775480   174 net.cpp:526] relu5 -> conv5 (in-place)\n",
      "I0618 07:43:35.775497   174 net.cpp:259] Setting up relu5\n",
      "I0618 07:43:35.775512   174 net.cpp:266] TEST Top shape for layer 14 'relu5' 1 256 13 13 (43264)\n",
      "I0618 07:43:35.775528   174 layer_factory.hpp:172] Creating layer 'pool5' of type 'Pooling'\n",
      "I0618 07:43:35.775538   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.775555   174 net.cpp:199] Created Layer pool5 (15)\n",
      "I0618 07:43:35.775569   174 net.cpp:571] pool5 <- conv5\n",
      "I0618 07:43:35.775583   174 net.cpp:541] pool5 -> pool5\n",
      "I0618 07:43:35.775665   174 net.cpp:259] Setting up pool5\n",
      "I0618 07:43:35.775688   174 net.cpp:266] TEST Top shape for layer 15 'pool5' 1 256 6 6 (9216)\n",
      "I0618 07:43:35.775701   174 layer_factory.hpp:172] Creating layer 'fc6' of type 'InnerProduct'\n",
      "I0618 07:43:35.775710   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:35.775734   174 net.cpp:199] Created Layer fc6 (16)\n",
      "I0618 07:43:35.775748   174 net.cpp:571] fc6 <- pool5\n",
      "I0618 07:43:35.775758   174 net.cpp:541] fc6 -> fc6\n",
      "I0618 07:43:36.449095   174 net.cpp:259] Setting up fc6\n",
      "I0618 07:43:36.449146   174 net.cpp:266] TEST Top shape for layer 16 'fc6' 1 4096 (4096)\n",
      "I0618 07:43:36.449175   174 layer_factory.hpp:172] Creating layer 'relu6' of type 'ReLU'\n",
      "I0618 07:43:36.449193   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:36.449213   174 net.cpp:199] Created Layer relu6 (17)\n",
      "I0618 07:43:36.449230   174 net.cpp:571] relu6 <- fc6\n",
      "I0618 07:43:36.449245   174 net.cpp:526] relu6 -> fc6 (in-place)\n",
      "I0618 07:43:36.449270   174 net.cpp:259] Setting up relu6\n",
      "I0618 07:43:36.449290   174 net.cpp:266] TEST Top shape for layer 17 'relu6' 1 4096 (4096)\n",
      "I0618 07:43:36.449299   174 layer_factory.hpp:172] Creating layer 'drop6' of type 'Dropout'\n",
      "I0618 07:43:36.449316   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:36.449333   174 net.cpp:199] Created Layer drop6 (18)\n",
      "I0618 07:43:36.449347   174 net.cpp:571] drop6 <- fc6\n",
      "I0618 07:43:36.449357   174 net.cpp:526] drop6 -> fc6 (in-place)\n",
      "I0618 07:43:36.483752   174 net.cpp:259] Setting up drop6\n",
      "I0618 07:43:36.483805   174 net.cpp:266] TEST Top shape for layer 18 'drop6' 1 4096 (4096)\n",
      "I0618 07:43:36.483820   174 layer_factory.hpp:172] Creating layer 'fc7' of type 'InnerProduct'\n",
      "I0618 07:43:36.483840   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:36.483866   174 net.cpp:199] Created Layer fc7 (19)\n",
      "I0618 07:43:36.483908   174 net.cpp:571] fc7 <- fc6\n",
      "I0618 07:43:36.483923   174 net.cpp:541] fc7 -> fc7\n",
      "I0618 07:43:36.786794   174 net.cpp:259] Setting up fc7\n",
      "I0618 07:43:36.786845   174 net.cpp:266] TEST Top shape for layer 19 'fc7' 1 4096 (4096)\n",
      "I0618 07:43:36.786883   174 layer_factory.hpp:172] Creating layer 'relu7' of type 'ReLU'\n",
      "I0618 07:43:36.786900   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:36.786921   174 net.cpp:199] Created Layer relu7 (20)\n",
      "I0618 07:43:36.786936   174 net.cpp:571] relu7 <- fc7\n",
      "I0618 07:43:36.786950   174 net.cpp:526] relu7 -> fc7 (in-place)\n",
      "I0618 07:43:36.786972   174 net.cpp:259] Setting up relu7\n",
      "I0618 07:43:36.786984   174 net.cpp:266] TEST Top shape for layer 20 'relu7' 1 4096 (4096)\n",
      "I0618 07:43:36.786996   174 layer_factory.hpp:172] Creating layer 'drop7' of type 'Dropout'\n",
      "I0618 07:43:36.787008   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:36.787024   174 net.cpp:199] Created Layer drop7 (21)\n",
      "I0618 07:43:36.787034   174 net.cpp:571] drop7 <- fc7\n",
      "I0618 07:43:36.787045   174 net.cpp:526] drop7 -> fc7 (in-place)\n",
      "I0618 07:43:36.821769   174 net.cpp:259] Setting up drop7\n",
      "I0618 07:43:36.821802   174 net.cpp:266] TEST Top shape for layer 21 'drop7' 1 4096 (4096)\n",
      "I0618 07:43:36.821820   174 layer_factory.hpp:172] Creating layer 'fc8' of type 'InnerProduct'\n",
      "I0618 07:43:36.821841   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:36.821859   174 net.cpp:199] Created Layer fc8 (22)\n",
      "I0618 07:43:36.821872   174 net.cpp:571] fc8 <- fc7\n",
      "I0618 07:43:36.821882   174 net.cpp:541] fc8 -> fc8\n",
      "I0618 07:43:36.822825   174 net.cpp:259] Setting up fc8\n",
      "I0618 07:43:36.822849   174 net.cpp:266] TEST Top shape for layer 22 'fc8' 1 2 (2)\n",
      "I0618 07:43:36.822871   174 layer_factory.hpp:172] Creating layer 'softmax' of type 'Softmax'\n",
      "I0618 07:43:36.822890   174 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:36.822917   174 net.cpp:199] Created Layer softmax (23)\n",
      "I0618 07:43:36.822930   174 net.cpp:571] softmax <- fc8\n",
      "I0618 07:43:36.822942   174 net.cpp:541] softmax -> softmax\n",
      "I0618 07:43:36.823030   174 net.cpp:259] Setting up softmax\n",
      "I0618 07:43:36.823046   174 net.cpp:266] TEST Top shape for layer 23 'softmax' 1 2 (2)\n",
      "I0618 07:43:36.823058   174 net.cpp:337] softmax does not need backward computation.\n",
      "I0618 07:43:36.823069   174 net.cpp:337] fc8 does not need backward computation.\n",
      "I0618 07:43:36.823081   174 net.cpp:337] drop7 does not need backward computation.\n",
      "I0618 07:43:36.823094   174 net.cpp:337] relu7 does not need backward computation.\n",
      "I0618 07:43:36.823101   174 net.cpp:337] fc7 does not need backward computation.\n",
      "I0618 07:43:36.823110   174 net.cpp:337] drop6 does not need backward computation.\n",
      "I0618 07:43:36.823117   174 net.cpp:337] relu6 does not need backward computation.\n",
      "I0618 07:43:36.823127   174 net.cpp:337] fc6 does not need backward computation.\n",
      "I0618 07:43:36.823132   174 net.cpp:337] pool5 does not need backward computation.\n",
      "I0618 07:43:36.823143   174 net.cpp:337] relu5 does not need backward computation.\n",
      "I0618 07:43:36.823149   174 net.cpp:337] conv5 does not need backward computation.\n",
      "I0618 07:43:36.823160   174 net.cpp:337] relu4 does not need backward computation.\n",
      "I0618 07:43:36.823166   174 net.cpp:337] conv4 does not need backward computation.\n",
      "I0618 07:43:36.823175   174 net.cpp:337] relu3 does not need backward computation.\n",
      "I0618 07:43:36.823194   174 net.cpp:337] conv3 does not need backward computation.\n",
      "I0618 07:43:36.823206   174 net.cpp:337] pool2 does not need backward computation.\n",
      "I0618 07:43:36.823215   174 net.cpp:337] norm2 does not need backward computation.\n",
      "I0618 07:43:36.823225   174 net.cpp:337] relu2 does not need backward computation.\n",
      "I0618 07:43:36.823233   174 net.cpp:337] conv2 does not need backward computation.\n",
      "I0618 07:43:36.823249   174 net.cpp:337] pool1 does not need backward computation.\n",
      "I0618 07:43:36.823287   174 net.cpp:337] norm1 does not need backward computation.\n",
      "I0618 07:43:36.823298   174 net.cpp:337] relu1 does not need backward computation.\n",
      "I0618 07:43:36.823308   174 net.cpp:337] conv1 does not need backward computation.\n",
      "I0618 07:43:36.823320   174 net.cpp:337] input does not need backward computation.\n",
      "I0618 07:43:36.823331   174 net.cpp:379] This network produces output softmax\n",
      "I0618 07:43:36.823357   174 net.cpp:402] Top memory (TEST) required for data: 8315264 diff: 8315264\n",
      "I0618 07:43:36.823369   174 net.cpp:405] Bottom memory (TEST) required for data: 8315256 diff: 8315256\n",
      "I0618 07:43:36.823374   174 net.cpp:408] Shared (in-place) memory (TEST) by data: 2665856 diff: 2665856\n",
      "I0618 07:43:36.823385   174 net.cpp:411] Parameters memory (TEST) required for data: 227505672 diff: 227505672\n",
      "I0618 07:43:36.823390   174 net.cpp:414] Parameters shared memory (TEST) by data: 0 diff: 0\n",
      "I0618 07:43:36.823400   174 net.cpp:420] Network initialization done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0618 07:43:36.933459   174 net.cpp:1129] Ignoring source layer train-data\n",
      "I0618 07:43:36.933499   174 net.cpp:1137] Copying source layer conv1 Type:Convolution #blobs=2\n",
      "I0618 07:43:36.933619   174 net.cpp:1137] Copying source layer relu1 Type:ReLU #blobs=0\n",
      "I0618 07:43:36.933635   174 net.cpp:1137] Copying source layer norm1 Type:LRN #blobs=0\n",
      "I0618 07:43:36.933645   174 net.cpp:1137] Copying source layer pool1 Type:Pooling #blobs=0\n",
      "I0618 07:43:36.933655   174 net.cpp:1137] Copying source layer conv2 Type:Convolution #blobs=2\n",
      "I0618 07:43:36.933849   174 net.cpp:1137] Copying source layer relu2 Type:ReLU #blobs=0\n",
      "I0618 07:43:36.933868   174 net.cpp:1137] Copying source layer norm2 Type:LRN #blobs=0\n",
      "I0618 07:43:36.933882   174 net.cpp:1137] Copying source layer pool2 Type:Pooling #blobs=0\n",
      "I0618 07:43:36.933889   174 net.cpp:1137] Copying source layer conv3 Type:Convolution #blobs=2\n",
      "I0618 07:43:36.934370   174 net.cpp:1137] Copying source layer relu3 Type:ReLU #blobs=0\n",
      "I0618 07:43:36.934388   174 net.cpp:1137] Copying source layer conv4 Type:Convolution #blobs=2\n",
      "I0618 07:43:36.934753   174 net.cpp:1137] Copying source layer relu4 Type:ReLU #blobs=0\n",
      "I0618 07:43:36.934772   174 net.cpp:1137] Copying source layer conv5 Type:Convolution #blobs=2\n",
      "I0618 07:43:36.935040   174 net.cpp:1137] Copying source layer relu5 Type:ReLU #blobs=0\n",
      "I0618 07:43:36.935053   174 net.cpp:1137] Copying source layer pool5 Type:Pooling #blobs=0\n",
      "I0618 07:43:36.935062   174 net.cpp:1137] Copying source layer fc6 Type:InnerProduct #blobs=2\n",
      "I0618 07:43:36.952936   174 net.cpp:1137] Copying source layer relu6 Type:ReLU #blobs=0\n",
      "I0618 07:43:36.952970   174 net.cpp:1137] Copying source layer drop6 Type:Dropout #blobs=0\n",
      "I0618 07:43:36.952980   174 net.cpp:1137] Copying source layer fc7 Type:InnerProduct #blobs=2\n",
      "I0618 07:43:36.960727   174 net.cpp:1137] Copying source layer relu7 Type:ReLU #blobs=0\n",
      "I0618 07:43:36.960752   174 net.cpp:1137] Copying source layer drop7 Type:Dropout #blobs=0\n",
      "I0618 07:43:36.960757   174 net.cpp:1137] Copying source layer fc8 Type:InnerProduct #blobs=2\n",
      "I0618 07:43:36.960809   174 net.cpp:1129] Ignoring source layer loss\n",
      "whale\n"
     ]
    }
   ],
   "source": [
    "!python submission.py '/dli/data/whale/data/train/face/w_1.jpg'  #This should return \"whale\" at the very bottom of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0618 07:43:50.606047   189 gpu_memory.cpp:105] GPUMemory::Manager initialized\n",
      "I0618 07:43:50.606840   189 gpu_memory.cpp:107] Total memory: 11996954624, Free: 11791564800, dev_info[0]: total=11996954624 free=11791564800\n",
      "W0618 07:43:50.606916   189 _caffe.cpp:172] DEPRECATION WARNING - deprecated use of Python interface\n",
      "W0618 07:43:50.607045   189 _caffe.cpp:173] Use this instead (with the named \"weights\" parameter):\n",
      "W0618 07:43:50.607060   189 _caffe.cpp:175] Net('/dli/data/digits/20190618-072806-d33f/deploy.prototxt', 1, weights='/dli/data/digits/20190618-072806-d33f/snapshot_iter_1620.caffemodel')\n",
      "I0618 07:43:50.607390   189 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: /dli/data/digits/20190618-072806-d33f/deploy.prototxt\n",
      "I0618 07:43:50.607432   189 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.\n",
      "W0618 07:43:50.607447   189 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.\n",
      "I0618 07:43:50.616987   189 net.cpp:79] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"input\"\n",
      "  type: \"Input\"\n",
      "  top: \"data\"\n",
      "  input_param {\n",
      "    shape {\n",
      "      dim: 1\n",
      "      dim: 3\n",
      "      dim: 227\n",
      "      dim: 227\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 96\n",
      "    kernel_size: 11\n",
      "    stride: 4\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"conv1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv4\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv4\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu4\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv4\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv5\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv5\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu5\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"conv5\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool5\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"pool5\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc6\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool5\"\n",
      "  top: \"fc6\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu6\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop6\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc7\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc7\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu7\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop7\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc8\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc8\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"softmax\"\n",
      "  type: \"Softmax\"\n",
      "  bottom: \"fc8\"\n",
      "  top: \"softmax\"\n",
      "}\n",
      "I0618 07:43:50.617462   189 net.cpp:109] Using FLOAT as default forward math type\n",
      "I0618 07:43:50.617480   189 net.cpp:115] Using FLOAT as default backward math type\n",
      "I0618 07:43:50.617499   189 layer_factory.hpp:172] Creating layer 'input' of type 'Input'\n",
      "I0618 07:43:50.617517   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:50.617545   189 net.cpp:199] Created Layer input (0)\n",
      "I0618 07:43:50.617563   189 net.cpp:541] input -> data\n",
      "I0618 07:43:50.618242   189 net.cpp:259] Setting up input\n",
      "I0618 07:43:50.618288   189 net.cpp:266] TEST Top shape for layer 0 'input' 1 3 227 227 (154587)\n",
      "I0618 07:43:50.618310   189 layer_factory.hpp:172] Creating layer 'conv1' of type 'Convolution'\n",
      "I0618 07:43:50.618321   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:50.618360   189 net.cpp:199] Created Layer conv1 (1)\n",
      "I0618 07:43:50.618373   189 net.cpp:571] conv1 <- data\n",
      "I0618 07:43:50.618382   189 net.cpp:541] conv1 -> conv1\n",
      "I0618 07:43:51.135170   189 net.cpp:259] Setting up conv1\n",
      "I0618 07:43:51.135215   189 net.cpp:266] TEST Top shape for layer 1 'conv1' 1 96 55 55 (290400)\n",
      "I0618 07:43:51.135237   189 layer_factory.hpp:172] Creating layer 'relu1' of type 'ReLU'\n",
      "I0618 07:43:51.135254   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.135269   189 net.cpp:199] Created Layer relu1 (2)\n",
      "I0618 07:43:51.135282   189 net.cpp:571] relu1 <- conv1\n",
      "I0618 07:43:51.135289   189 net.cpp:526] relu1 -> conv1 (in-place)\n",
      "I0618 07:43:51.135310   189 net.cpp:259] Setting up relu1\n",
      "I0618 07:43:51.135323   189 net.cpp:266] TEST Top shape for layer 2 'relu1' 1 96 55 55 (290400)\n",
      "I0618 07:43:51.135336   189 layer_factory.hpp:172] Creating layer 'norm1' of type 'LRN'\n",
      "I0618 07:43:51.135354   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.135385   189 net.cpp:199] Created Layer norm1 (3)\n",
      "I0618 07:43:51.135396   189 net.cpp:571] norm1 <- conv1\n",
      "I0618 07:43:51.135403   189 net.cpp:541] norm1 -> norm1\n",
      "I0618 07:43:51.135466   189 net.cpp:259] Setting up norm1\n",
      "I0618 07:43:51.135483   189 net.cpp:266] TEST Top shape for layer 3 'norm1' 1 96 55 55 (290400)\n",
      "I0618 07:43:51.135489   189 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'\n",
      "I0618 07:43:51.135496   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.135538   189 net.cpp:199] Created Layer pool1 (4)\n",
      "I0618 07:43:51.135551   189 net.cpp:571] pool1 <- norm1\n",
      "I0618 07:43:51.135558   189 net.cpp:541] pool1 -> pool1\n",
      "I0618 07:43:51.135625   189 net.cpp:259] Setting up pool1\n",
      "I0618 07:43:51.135641   189 net.cpp:266] TEST Top shape for layer 4 'pool1' 1 96 27 27 (69984)\n",
      "I0618 07:43:51.135650   189 layer_factory.hpp:172] Creating layer 'conv2' of type 'Convolution'\n",
      "I0618 07:43:51.135661   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.135677   189 net.cpp:199] Created Layer conv2 (5)\n",
      "I0618 07:43:51.135689   189 net.cpp:571] conv2 <- pool1\n",
      "I0618 07:43:51.135695   189 net.cpp:541] conv2 -> conv2\n",
      "I0618 07:43:51.142767   189 net.cpp:259] Setting up conv2\n",
      "I0618 07:43:51.142794   189 net.cpp:266] TEST Top shape for layer 5 'conv2' 1 256 27 27 (186624)\n",
      "I0618 07:43:51.142822   189 layer_factory.hpp:172] Creating layer 'relu2' of type 'ReLU'\n",
      "I0618 07:43:51.142841   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.142860   189 net.cpp:199] Created Layer relu2 (6)\n",
      "I0618 07:43:51.142876   189 net.cpp:571] relu2 <- conv2\n",
      "I0618 07:43:51.142892   189 net.cpp:526] relu2 -> conv2 (in-place)\n",
      "I0618 07:43:51.142915   189 net.cpp:259] Setting up relu2\n",
      "I0618 07:43:51.142930   189 net.cpp:266] TEST Top shape for layer 6 'relu2' 1 256 27 27 (186624)\n",
      "I0618 07:43:51.142946   189 layer_factory.hpp:172] Creating layer 'norm2' of type 'LRN'\n",
      "I0618 07:43:51.142961   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.142985   189 net.cpp:199] Created Layer norm2 (7)\n",
      "I0618 07:43:51.142999   189 net.cpp:571] norm2 <- conv2\n",
      "I0618 07:43:51.143014   189 net.cpp:541] norm2 -> norm2\n",
      "I0618 07:43:51.143077   189 net.cpp:259] Setting up norm2\n",
      "I0618 07:43:51.143096   189 net.cpp:266] TEST Top shape for layer 7 'norm2' 1 256 27 27 (186624)\n",
      "I0618 07:43:51.143112   189 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'\n",
      "I0618 07:43:51.143127   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.143148   189 net.cpp:199] Created Layer pool2 (8)\n",
      "I0618 07:43:51.143163   189 net.cpp:571] pool2 <- norm2\n",
      "I0618 07:43:51.143179   189 net.cpp:541] pool2 -> pool2\n",
      "I0618 07:43:51.143244   189 net.cpp:259] Setting up pool2\n",
      "I0618 07:43:51.143262   189 net.cpp:266] TEST Top shape for layer 8 'pool2' 1 256 13 13 (43264)\n",
      "I0618 07:43:51.143280   189 layer_factory.hpp:172] Creating layer 'conv3' of type 'Convolution'\n",
      "I0618 07:43:51.143296   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.143321   189 net.cpp:199] Created Layer conv3 (9)\n",
      "I0618 07:43:51.143335   189 net.cpp:571] conv3 <- pool2\n",
      "I0618 07:43:51.143352   189 net.cpp:541] conv3 -> conv3\n",
      "I0618 07:43:51.159070   189 net.cpp:259] Setting up conv3\n",
      "I0618 07:43:51.159098   189 net.cpp:266] TEST Top shape for layer 9 'conv3' 1 384 13 13 (64896)\n",
      "I0618 07:43:51.159124   189 layer_factory.hpp:172] Creating layer 'relu3' of type 'ReLU'\n",
      "I0618 07:43:51.159138   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.159147   189 net.cpp:199] Created Layer relu3 (10)\n",
      "I0618 07:43:51.159158   189 net.cpp:571] relu3 <- conv3\n",
      "I0618 07:43:51.159165   189 net.cpp:526] relu3 -> conv3 (in-place)\n",
      "I0618 07:43:51.159180   189 net.cpp:259] Setting up relu3\n",
      "I0618 07:43:51.159193   189 net.cpp:266] TEST Top shape for layer 10 'relu3' 1 384 13 13 (64896)\n",
      "I0618 07:43:51.159200   189 layer_factory.hpp:172] Creating layer 'conv4' of type 'Convolution'\n",
      "I0618 07:43:51.159210   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.159226   189 net.cpp:199] Created Layer conv4 (11)\n",
      "I0618 07:43:51.159237   189 net.cpp:571] conv4 <- conv3\n",
      "I0618 07:43:51.159245   189 net.cpp:541] conv4 -> conv4\n",
      "I0618 07:43:51.171582   189 net.cpp:259] Setting up conv4\n",
      "I0618 07:43:51.171615   189 net.cpp:266] TEST Top shape for layer 11 'conv4' 1 384 13 13 (64896)\n",
      "I0618 07:43:51.171672   189 layer_factory.hpp:172] Creating layer 'relu4' of type 'ReLU'\n",
      "I0618 07:43:51.171690   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.171710   189 net.cpp:199] Created Layer relu4 (12)\n",
      "I0618 07:43:51.171723   189 net.cpp:571] relu4 <- conv4\n",
      "I0618 07:43:51.171730   189 net.cpp:526] relu4 -> conv4 (in-place)\n",
      "I0618 07:43:51.171746   189 net.cpp:259] Setting up relu4\n",
      "I0618 07:43:51.171759   189 net.cpp:266] TEST Top shape for layer 12 'relu4' 1 384 13 13 (64896)\n",
      "I0618 07:43:51.171768   189 layer_factory.hpp:172] Creating layer 'conv5' of type 'Convolution'\n",
      "I0618 07:43:51.171779   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.171794   189 net.cpp:199] Created Layer conv5 (13)\n",
      "I0618 07:43:51.171804   189 net.cpp:571] conv5 <- conv4\n",
      "I0618 07:43:51.171811   189 net.cpp:541] conv5 -> conv5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0618 07:43:51.180016   189 net.cpp:259] Setting up conv5\n",
      "I0618 07:43:51.180042   189 net.cpp:266] TEST Top shape for layer 13 'conv5' 1 256 13 13 (43264)\n",
      "I0618 07:43:51.180056   189 layer_factory.hpp:172] Creating layer 'relu5' of type 'ReLU'\n",
      "I0618 07:43:51.180071   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.180080   189 net.cpp:199] Created Layer relu5 (14)\n",
      "I0618 07:43:51.180088   189 net.cpp:571] relu5 <- conv5\n",
      "I0618 07:43:51.180096   189 net.cpp:526] relu5 -> conv5 (in-place)\n",
      "I0618 07:43:51.180109   189 net.cpp:259] Setting up relu5\n",
      "I0618 07:43:51.180122   189 net.cpp:266] TEST Top shape for layer 14 'relu5' 1 256 13 13 (43264)\n",
      "I0618 07:43:51.180130   189 layer_factory.hpp:172] Creating layer 'pool5' of type 'Pooling'\n",
      "I0618 07:43:51.180140   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.180152   189 net.cpp:199] Created Layer pool5 (15)\n",
      "I0618 07:43:51.180162   189 net.cpp:571] pool5 <- conv5\n",
      "I0618 07:43:51.180169   189 net.cpp:541] pool5 -> pool5\n",
      "I0618 07:43:51.180234   189 net.cpp:259] Setting up pool5\n",
      "I0618 07:43:51.180250   189 net.cpp:266] TEST Top shape for layer 15 'pool5' 1 256 6 6 (9216)\n",
      "I0618 07:43:51.180258   189 layer_factory.hpp:172] Creating layer 'fc6' of type 'InnerProduct'\n",
      "I0618 07:43:51.180270   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.180282   189 net.cpp:199] Created Layer fc6 (16)\n",
      "I0618 07:43:51.180294   189 net.cpp:571] fc6 <- pool5\n",
      "I0618 07:43:51.180300   189 net.cpp:541] fc6 -> fc6\n",
      "I0618 07:43:51.857597   189 net.cpp:259] Setting up fc6\n",
      "I0618 07:43:51.857643   189 net.cpp:266] TEST Top shape for layer 16 'fc6' 1 4096 (4096)\n",
      "I0618 07:43:51.857671   189 layer_factory.hpp:172] Creating layer 'relu6' of type 'ReLU'\n",
      "I0618 07:43:51.857686   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.857697   189 net.cpp:199] Created Layer relu6 (17)\n",
      "I0618 07:43:51.857709   189 net.cpp:571] relu6 <- fc6\n",
      "I0618 07:43:51.857722   189 net.cpp:526] relu6 -> fc6 (in-place)\n",
      "I0618 07:43:51.857740   189 net.cpp:259] Setting up relu6\n",
      "I0618 07:43:51.857753   189 net.cpp:266] TEST Top shape for layer 17 'relu6' 1 4096 (4096)\n",
      "I0618 07:43:51.857758   189 layer_factory.hpp:172] Creating layer 'drop6' of type 'Dropout'\n",
      "I0618 07:43:51.857770   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.857794   189 net.cpp:199] Created Layer drop6 (18)\n",
      "I0618 07:43:51.857805   189 net.cpp:571] drop6 <- fc6\n",
      "I0618 07:43:51.857811   189 net.cpp:526] drop6 -> fc6 (in-place)\n",
      "I0618 07:43:51.892110   189 net.cpp:259] Setting up drop6\n",
      "I0618 07:43:51.892139   189 net.cpp:266] TEST Top shape for layer 18 'drop6' 1 4096 (4096)\n",
      "I0618 07:43:51.892155   189 layer_factory.hpp:172] Creating layer 'fc7' of type 'InnerProduct'\n",
      "I0618 07:43:51.892174   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:51.892196   189 net.cpp:199] Created Layer fc7 (19)\n",
      "I0618 07:43:51.892235   189 net.cpp:571] fc7 <- fc6\n",
      "I0618 07:43:51.892249   189 net.cpp:541] fc7 -> fc7\n",
      "I0618 07:43:52.192952   189 net.cpp:259] Setting up fc7\n",
      "I0618 07:43:52.193006   189 net.cpp:266] TEST Top shape for layer 19 'fc7' 1 4096 (4096)\n",
      "I0618 07:43:52.193034   189 layer_factory.hpp:172] Creating layer 'relu7' of type 'ReLU'\n",
      "I0618 07:43:52.193054   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:52.193074   189 net.cpp:199] Created Layer relu7 (20)\n",
      "I0618 07:43:52.193090   189 net.cpp:571] relu7 <- fc7\n",
      "I0618 07:43:52.193105   189 net.cpp:526] relu7 -> fc7 (in-place)\n",
      "I0618 07:43:52.193132   189 net.cpp:259] Setting up relu7\n",
      "I0618 07:43:52.193148   189 net.cpp:266] TEST Top shape for layer 20 'relu7' 1 4096 (4096)\n",
      "I0618 07:43:52.193161   189 layer_factory.hpp:172] Creating layer 'drop7' of type 'Dropout'\n",
      "I0618 07:43:52.193176   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:52.193193   189 net.cpp:199] Created Layer drop7 (21)\n",
      "I0618 07:43:52.193205   189 net.cpp:571] drop7 <- fc7\n",
      "I0618 07:43:52.193222   189 net.cpp:526] drop7 -> fc7 (in-place)\n",
      "I0618 07:43:52.227685   189 net.cpp:259] Setting up drop7\n",
      "I0618 07:43:52.227713   189 net.cpp:266] TEST Top shape for layer 21 'drop7' 1 4096 (4096)\n",
      "I0618 07:43:52.227730   189 layer_factory.hpp:172] Creating layer 'fc8' of type 'InnerProduct'\n",
      "I0618 07:43:52.227749   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:52.227773   189 net.cpp:199] Created Layer fc8 (22)\n",
      "I0618 07:43:52.227787   189 net.cpp:571] fc8 <- fc7\n",
      "I0618 07:43:52.227805   189 net.cpp:541] fc8 -> fc8\n",
      "I0618 07:43:52.228735   189 net.cpp:259] Setting up fc8\n",
      "I0618 07:43:52.228762   189 net.cpp:266] TEST Top shape for layer 22 'fc8' 1 2 (2)\n",
      "I0618 07:43:52.228786   189 layer_factory.hpp:172] Creating layer 'softmax' of type 'Softmax'\n",
      "I0618 07:43:52.228801   189 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0618 07:43:52.228816   189 net.cpp:199] Created Layer softmax (23)\n",
      "I0618 07:43:52.228827   189 net.cpp:571] softmax <- fc8\n",
      "I0618 07:43:52.228834   189 net.cpp:541] softmax -> softmax\n",
      "I0618 07:43:52.228920   189 net.cpp:259] Setting up softmax\n",
      "I0618 07:43:52.228935   189 net.cpp:266] TEST Top shape for layer 23 'softmax' 1 2 (2)\n",
      "I0618 07:43:52.228943   189 net.cpp:337] softmax does not need backward computation.\n",
      "I0618 07:43:52.228951   189 net.cpp:337] fc8 does not need backward computation.\n",
      "I0618 07:43:52.228961   189 net.cpp:337] drop7 does not need backward computation.\n",
      "I0618 07:43:52.228967   189 net.cpp:337] relu7 does not need backward computation.\n",
      "I0618 07:43:52.228977   189 net.cpp:337] fc7 does not need backward computation.\n",
      "I0618 07:43:52.228982   189 net.cpp:337] drop6 does not need backward computation.\n",
      "I0618 07:43:52.228989   189 net.cpp:337] relu6 does not need backward computation.\n",
      "I0618 07:43:52.228999   189 net.cpp:337] fc6 does not need backward computation.\n",
      "I0618 07:43:52.229005   189 net.cpp:337] pool5 does not need backward computation.\n",
      "I0618 07:43:52.229012   189 net.cpp:337] relu5 does not need backward computation.\n",
      "I0618 07:43:52.229018   189 net.cpp:337] conv5 does not need backward computation.\n",
      "I0618 07:43:52.229030   189 net.cpp:337] relu4 does not need backward computation.\n",
      "I0618 07:43:52.229037   189 net.cpp:337] conv4 does not need backward computation.\n",
      "I0618 07:43:52.229041   189 net.cpp:337] relu3 does not need backward computation.\n",
      "I0618 07:43:52.229048   189 net.cpp:337] conv3 does not need backward computation.\n",
      "I0618 07:43:52.229056   189 net.cpp:337] pool2 does not need backward computation.\n",
      "I0618 07:43:52.229063   189 net.cpp:337] norm2 does not need backward computation.\n",
      "I0618 07:43:52.229073   189 net.cpp:337] relu2 does not need backward computation.\n",
      "I0618 07:43:52.229079   189 net.cpp:337] conv2 does not need backward computation.\n",
      "I0618 07:43:52.229084   189 net.cpp:337] pool1 does not need backward computation.\n",
      "I0618 07:43:52.229118   189 net.cpp:337] norm1 does not need backward computation.\n",
      "I0618 07:43:52.229127   189 net.cpp:337] relu1 does not need backward computation.\n",
      "I0618 07:43:52.229133   189 net.cpp:337] conv1 does not need backward computation.\n",
      "I0618 07:43:52.229146   189 net.cpp:337] input does not need backward computation.\n",
      "I0618 07:43:52.229151   189 net.cpp:379] This network produces output softmax\n",
      "I0618 07:43:52.229178   189 net.cpp:402] Top memory (TEST) required for data: 8315264 diff: 8315264\n",
      "I0618 07:43:52.229190   189 net.cpp:405] Bottom memory (TEST) required for data: 8315256 diff: 8315256\n",
      "I0618 07:43:52.229197   189 net.cpp:408] Shared (in-place) memory (TEST) by data: 2665856 diff: 2665856\n",
      "I0618 07:43:52.229207   189 net.cpp:411] Parameters memory (TEST) required for data: 227505672 diff: 227505672\n",
      "I0618 07:43:52.229212   189 net.cpp:414] Parameters shared memory (TEST) by data: 0 diff: 0\n",
      "I0618 07:43:52.229223   189 net.cpp:420] Network initialization done.\n",
      "I0618 07:43:52.335795   189 net.cpp:1129] Ignoring source layer train-data\n",
      "I0618 07:43:52.335832   189 net.cpp:1137] Copying source layer conv1 Type:Convolution #blobs=2\n",
      "I0618 07:43:52.335935   189 net.cpp:1137] Copying source layer relu1 Type:ReLU #blobs=0\n",
      "I0618 07:43:52.335950   189 net.cpp:1137] Copying source layer norm1 Type:LRN #blobs=0\n",
      "I0618 07:43:52.335958   189 net.cpp:1137] Copying source layer pool1 Type:Pooling #blobs=0\n",
      "I0618 07:43:52.335968   189 net.cpp:1137] Copying source layer conv2 Type:Convolution #blobs=2\n",
      "I0618 07:43:52.336148   189 net.cpp:1137] Copying source layer relu2 Type:ReLU #blobs=0\n",
      "I0618 07:43:52.336161   189 net.cpp:1137] Copying source layer norm2 Type:LRN #blobs=0\n",
      "I0618 07:43:52.336167   189 net.cpp:1137] Copying source layer pool2 Type:Pooling #blobs=0\n",
      "I0618 07:43:52.336174   189 net.cpp:1137] Copying source layer conv3 Type:Convolution #blobs=2\n",
      "I0618 07:43:52.336611   189 net.cpp:1137] Copying source layer relu3 Type:ReLU #blobs=0\n",
      "I0618 07:43:52.336627   189 net.cpp:1137] Copying source layer conv4 Type:Convolution #blobs=2\n",
      "I0618 07:43:52.336987   189 net.cpp:1137] Copying source layer relu4 Type:ReLU #blobs=0\n",
      "I0618 07:43:52.336997   189 net.cpp:1137] Copying source layer conv5 Type:Convolution #blobs=2\n",
      "I0618 07:43:52.337239   189 net.cpp:1137] Copying source layer relu5 Type:ReLU #blobs=0\n",
      "I0618 07:43:52.337255   189 net.cpp:1137] Copying source layer pool5 Type:Pooling #blobs=0\n",
      "I0618 07:43:52.337276   189 net.cpp:1137] Copying source layer fc6 Type:InnerProduct #blobs=2\n",
      "I0618 07:43:52.354903   189 net.cpp:1137] Copying source layer relu6 Type:ReLU #blobs=0\n",
      "I0618 07:43:52.354934   189 net.cpp:1137] Copying source layer drop6 Type:Dropout #blobs=0\n",
      "I0618 07:43:52.354948   189 net.cpp:1137] Copying source layer fc7 Type:InnerProduct #blobs=2\n",
      "I0618 07:43:52.362766   189 net.cpp:1137] Copying source layer relu7 Type:ReLU #blobs=0\n",
      "I0618 07:43:52.362789   189 net.cpp:1137] Copying source layer drop7 Type:Dropout #blobs=0\n",
      "I0618 07:43:52.362795   189 net.cpp:1137] Copying source layer fc8 Type:InnerProduct #blobs=2\n",
      "I0618 07:43:52.362843   189 net.cpp:1129] Ignoring source layer loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not whale\r\n"
     ]
    }
   ],
   "source": [
    "!python submission.py '/dli/data/whale/data/train/not_face/w_1.jpg'  #This should return \"not whale\" at the very bottom of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Assessment1.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
